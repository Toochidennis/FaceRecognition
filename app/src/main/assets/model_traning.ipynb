{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nfrom tqdm import tqdm\n#import libraries","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-23T13:24:35.361765Z","iopub.execute_input":"2022-12-23T13:24:35.362101Z","iopub.status.idle":"2022-12-23T13:24:35.762656Z","shell.execute_reply.started":"2022-12-23T13:24:35.362023Z","shell.execute_reply":"2022-12-23T13:24:35.761705Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data = os.listdir('../input/project/dataset/')\ndata","metadata":{"execution":{"iopub.status.busy":"2022-12-23T13:24:35.764994Z","iopub.execute_input":"2022-12-23T13:24:35.765377Z","iopub.status.idle":"2022-12-23T13:24:35.791616Z","shell.execute_reply.started":"2022-12-23T13:24:35.765329Z","shell.execute_reply":"2022-12-23T13:24:35.790553Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"['hardik_pandya',\n 'Simon_Helberg',\n 'celestine',\n 'scarlett_johansson',\n 'sylvester_stallone',\n 'messi',\n 'Jim_Parsons',\n 'random_person',\n 'samson',\n 'mohamed_ali',\n 'brad_pitt',\n 'ronaldo',\n 'Jennifer_Aniston',\n 'victor',\n 'dhoni',\n 'pewdiepie',\n 'blessing',\n 'godswill',\n 'Johnny_Galeck',\n 'suresh_raina']"},"metadata":{}}]},{"cell_type":"code","source":"imageArray = []\nlabelArray = []\n\npath = '../input/project/dataset/'\n\nfor i in range(len(data)):\n    subFiles = os.listdir(path+data[i])\n    for k in tqdm(range(len(subFiles))):\n        try:\n            images = cv2.imread(path+data[i]+\"/\"+subFiles[k])\n            images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB)\n            images = cv2.resize(images, (96, 96))\n            \n            imageArray.append(images)\n            labelArray.append(i)\n            \n        except:\n            pass\n            ","metadata":{"execution":{"iopub.status.busy":"2022-12-23T13:24:35.793076Z","iopub.execute_input":"2022-12-23T13:24:35.793433Z","iopub.status.idle":"2022-12-23T13:26:26.936801Z","shell.execute_reply.started":"2022-12-23T13:24:35.793398Z","shell.execute_reply":"2022-12-23T13:26:26.935798Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"100%|██████████| 292/292 [00:03<00:00, 84.12it/s]\n100%|██████████| 484/484 [00:06<00:00, 77.20it/s]\n100%|██████████| 7/7 [00:00<00:00, 92.36it/s]\n100%|██████████| 507/507 [00:07<00:00, 63.61it/s]\n100%|██████████| 480/480 [00:06<00:00, 79.54it/s]\n100%|██████████| 432/432 [00:05<00:00, 84.18it/s]\n100%|██████████| 639/639 [00:07<00:00, 80.61it/s]\n100%|██████████| 2250/2250 [00:26<00:00, 84.01it/s] \n100%|██████████| 5/5 [00:00<00:00, 87.12it/s]\n100%|██████████| 338/338 [00:04<00:00, 81.05it/s]\n100%|██████████| 552/552 [00:07<00:00, 72.39it/s]\n100%|██████████| 418/418 [00:05<00:00, 79.56it/s]\n100%|██████████| 639/639 [00:08<00:00, 71.85it/s]\n100%|██████████| 7/7 [00:00<00:00, 89.39it/s]\n100%|██████████| 314/314 [00:03<00:00, 93.79it/s]\n100%|██████████| 395/395 [00:04<00:00, 86.67it/s]\n100%|██████████| 4/4 [00:00<00:00, 104.51it/s]\n100%|██████████| 9/9 [00:00<00:00, 93.33it/s]\n100%|██████████| 508/508 [00:06<00:00, 74.57it/s]\n100%|██████████| 319/319 [00:03<00:00, 98.09it/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"import gc\ngc.collect()\n#clear memory","metadata":{"execution":{"iopub.status.busy":"2022-12-23T13:26:26.939221Z","iopub.execute_input":"2022-12-23T13:26:26.941212Z","iopub.status.idle":"2022-12-23T13:26:27.030786Z","shell.execute_reply.started":"2022-12-23T13:26:26.941182Z","shell.execute_reply":"2022-12-23T13:26:27.029672Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"23"},"metadata":{}}]},{"cell_type":"code","source":"imageArray[0]","metadata":{"execution":{"iopub.status.busy":"2022-12-23T13:26:27.032373Z","iopub.execute_input":"2022-12-23T13:26:27.033053Z","iopub.status.idle":"2022-12-23T13:26:27.046170Z","shell.execute_reply.started":"2022-12-23T13:26:27.033017Z","shell.execute_reply":"2022-12-23T13:26:27.045207Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"array([[[247, 234, 226],\n        [247, 234, 227],\n        [224, 209, 206],\n        ...,\n        [128, 102, 103],\n        [197, 169, 165],\n        [213, 183, 175]],\n\n       [[246, 233, 225],\n        [246, 233, 226],\n        [221, 206, 203],\n        ...,\n        [158, 131, 131],\n        [210, 180, 175],\n        [216, 185, 175]],\n\n       [[246, 233, 225],\n        [246, 234, 227],\n        [225, 210, 207],\n        ...,\n        [183, 154, 153],\n        [217, 185, 180],\n        [219, 185, 175]],\n\n       ...,\n\n       [[ 19,  16,  33],\n        [ 17,  14,  32],\n        [ 17,  16,  34],\n        ...,\n        [ 47,  40,  58],\n        [ 47,  40,  57],\n        [ 46,  39,  55]],\n\n       [[ 19,  16,  33],\n        [ 18,  15,  33],\n        [ 16,  15,  33],\n        ...,\n        [ 47,  39,  57],\n        [ 47,  39,  58],\n        [ 46,  38,  57]],\n\n       [[ 19,  16,  33],\n        [ 17,  14,  32],\n        [ 16,  15,  33],\n        ...,\n        [ 48,  39,  58],\n        [ 48,  39,  60],\n        [ 47,  38,  59]]], dtype=uint8)"},"metadata":{}}]},{"cell_type":"code","source":"labelArray[:10]","metadata":{"execution":{"iopub.status.busy":"2022-12-23T13:26:27.047784Z","iopub.execute_input":"2022-12-23T13:26:27.048146Z","iopub.status.idle":"2022-12-23T13:26:27.056256Z","shell.execute_reply.started":"2022-12-23T13:26:27.048107Z","shell.execute_reply":"2022-12-23T13:26:27.055178Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"},"metadata":{}}]},{"cell_type":"code","source":"imageArray = np.array(imageArray)/255.0\nlabelArray = np.array(labelArray)","metadata":{"execution":{"iopub.status.busy":"2022-12-23T13:26:27.057787Z","iopub.execute_input":"2022-12-23T13:26:27.058328Z","iopub.status.idle":"2022-12-23T13:26:27.873594Z","shell.execute_reply.started":"2022-12-23T13:26:27.058294Z","shell.execute_reply":"2022-12-23T13:26:27.872495Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"imageArray[0]","metadata":{"execution":{"iopub.status.busy":"2022-12-23T13:26:27.878858Z","iopub.execute_input":"2022-12-23T13:26:27.881384Z","iopub.status.idle":"2022-12-23T13:26:27.895001Z","shell.execute_reply.started":"2022-12-23T13:26:27.881330Z","shell.execute_reply":"2022-12-23T13:26:27.893917Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array([[[0.96862745, 0.91764706, 0.88627451],\n        [0.96862745, 0.91764706, 0.89019608],\n        [0.87843137, 0.81960784, 0.80784314],\n        ...,\n        [0.50196078, 0.4       , 0.40392157],\n        [0.77254902, 0.6627451 , 0.64705882],\n        [0.83529412, 0.71764706, 0.68627451]],\n\n       [[0.96470588, 0.91372549, 0.88235294],\n        [0.96470588, 0.91372549, 0.88627451],\n        [0.86666667, 0.80784314, 0.79607843],\n        ...,\n        [0.61960784, 0.51372549, 0.51372549],\n        [0.82352941, 0.70588235, 0.68627451],\n        [0.84705882, 0.7254902 , 0.68627451]],\n\n       [[0.96470588, 0.91372549, 0.88235294],\n        [0.96470588, 0.91764706, 0.89019608],\n        [0.88235294, 0.82352941, 0.81176471],\n        ...,\n        [0.71764706, 0.60392157, 0.6       ],\n        [0.85098039, 0.7254902 , 0.70588235],\n        [0.85882353, 0.7254902 , 0.68627451]],\n\n       ...,\n\n       [[0.0745098 , 0.0627451 , 0.12941176],\n        [0.06666667, 0.05490196, 0.1254902 ],\n        [0.06666667, 0.0627451 , 0.13333333],\n        ...,\n        [0.18431373, 0.15686275, 0.22745098],\n        [0.18431373, 0.15686275, 0.22352941],\n        [0.18039216, 0.15294118, 0.21568627]],\n\n       [[0.0745098 , 0.0627451 , 0.12941176],\n        [0.07058824, 0.05882353, 0.12941176],\n        [0.0627451 , 0.05882353, 0.12941176],\n        ...,\n        [0.18431373, 0.15294118, 0.22352941],\n        [0.18431373, 0.15294118, 0.22745098],\n        [0.18039216, 0.14901961, 0.22352941]],\n\n       [[0.0745098 , 0.0627451 , 0.12941176],\n        [0.06666667, 0.05490196, 0.1254902 ],\n        [0.0627451 , 0.05882353, 0.12941176],\n        ...,\n        [0.18823529, 0.15294118, 0.22745098],\n        [0.18823529, 0.15294118, 0.23529412],\n        [0.18431373, 0.14901961, 0.23137255]]])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\n#clear memory","metadata":{"execution":{"iopub.status.busy":"2022-12-23T13:26:27.899556Z","iopub.execute_input":"2022-12-23T13:26:27.900312Z","iopub.status.idle":"2022-12-23T13:26:28.496643Z","shell.execute_reply.started":"2022-12-23T13:26:27.900278Z","shell.execute_reply":"2022-12-23T13:26:28.495101Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"X_train, x_test, Y_train, y_test = train_test_split(imageArray, labelArray, test_size=0.15) #20% for testing\n#clear memory","metadata":{"execution":{"iopub.status.busy":"2022-12-23T13:26:28.501550Z","iopub.execute_input":"2022-12-23T13:26:28.501886Z","iopub.status.idle":"2022-12-23T13:26:29.115594Z","shell.execute_reply.started":"2022-12-23T13:26:28.501857Z","shell.execute_reply":"2022-12-23T13:26:29.114402Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom keras import layers, optimizers, callbacks, applications\nfrom keras.callbacks import ModelCheckpoint\n","metadata":{"execution":{"iopub.status.busy":"2022-12-23T13:26:29.120864Z","iopub.execute_input":"2022-12-23T13:26:29.123237Z","iopub.status.idle":"2022-12-23T13:26:39.119294Z","shell.execute_reply.started":"2022-12-23T13:26:29.123166Z","shell.execute_reply":"2022-12-23T13:26:39.118177Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nefficient_net = tf.keras.applications.EfficientNetB0(\nweights= 'imagenet',\ninput_shape = (96,96,3),\ninclude_top = False)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-23T13:26:39.121066Z","iopub.execute_input":"2022-12-23T13:26:39.121781Z","iopub.status.idle":"2022-12-23T13:26:46.304491Z","shell.execute_reply.started":"2022-12-23T13:26:39.121738Z","shell.execute_reply":"2022-12-23T13:26:46.303550Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"2022-12-23 13:26:39.297282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:39.298263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:39.560506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:39.561437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:39.562237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:39.563068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:39.564865: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-23 13:26:39.797807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:39.798635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:39.799426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:39.800193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:39.801244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:39.802422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:44.152222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:44.153111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:44.153909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:44.154610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:44.155292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:44.155949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13789 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n2022-12-23 13:26:44.161527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 13:26:44.162213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13789 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n16711680/16705208 [==============================] - 0s 0us/step\n16719872/16705208 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(efficient_net)\nmodel.add(layers.GlobalAveragePooling2D())\nmodel.add(Dropout(0.3))\nmodel.add(layers.Dense(1))\nmodel.summary()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-23T13:26:46.305864Z","iopub.execute_input":"2022-12-23T13:26:46.306214Z","iopub.status.idle":"2022-12-23T13:26:47.068439Z","shell.execute_reply.started":"2022-12-23T13:26:46.306179Z","shell.execute_reply":"2022-12-23T13:26:47.067418Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nefficientnetb0 (Functional)  (None, 3, 3, 1280)        4049571   \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 1280)              0         \n_________________________________________________________________\ndropout (Dropout)            (None, 1280)              0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 1281      \n=================================================================\nTotal params: 4,050,852\nTrainable params: 4,008,829\nNon-trainable params: 42,023\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from tensorflow.keras.optimizers import Adam\n#model.compile(optimizer = Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\nmodel.compile(optimizer=\"adam\", loss='mean_squared_error', metrics=['mae'])","metadata":{"execution":{"iopub.status.busy":"2022-12-23T13:26:47.070062Z","iopub.execute_input":"2022-12-23T13:26:47.070704Z","iopub.status.idle":"2022-12-23T13:26:47.086046Z","shell.execute_reply.started":"2022-12-23T13:26:47.070664Z","shell.execute_reply":"2022-12-23T13:26:47.085125Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_filepath = 'trained_model/best_model'\n\ncustom_callbacks = [\n    tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_mae',\n    factor=0.9,\n    patience=5,\n    verbose=1,\n    mode='auto',\n    cooldown=0,\n    min_lr=1e-6\n   \n),\n    ModelCheckpoint(\n        filepath = checkpoint_filepath,\n        monitor = 'val_mae',\n        mode = 'auto',\n        verbose = 1,\n        save_best_only = True,\n        save_weights_only = True\n    )\n]","metadata":{"execution":{"iopub.status.busy":"2022-12-23T13:26:47.087732Z","iopub.execute_input":"2022-12-23T13:26:47.088467Z","iopub.status.idle":"2022-12-23T13:26:47.095146Z","shell.execute_reply.started":"2022-12-23T13:26:47.088431Z","shell.execute_reply":"2022-12-23T13:26:47.094109Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nstart_time = time.time()\n\nnum_epochs = 300\nbatch_size =64\n\nhistory = model.fit(\n    X_train, Y_train,\n    epochs = num_epochs,\n    batch_size= batch_size,\n    validation_data = (x_test, y_test),\n    callbacks = custom_callbacks\n)\n\nprint(history.history)\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2022-12-23T13:26:47.096606Z","iopub.execute_input":"2022-12-23T13:26:47.097066Z","iopub.status.idle":"2022-12-23T14:40:01.272698Z","shell.execute_reply.started":"2022-12-23T13:26:47.097032Z","shell.execute_reply":"2022-12-23T14:40:01.271656Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"2022-12-23 13:26:47.400139: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 808316928 exceeds 10% of free system memory.\n2022-12-23 13:26:48.333839: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 808316928 exceeds 10% of free system memory.\n2022-12-23 13:26:49.010671: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/300\n","output_type":"stream"},{"name":"stderr","text":"2022-12-23 13:26:58.413446: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"115/115 [==============================] - 38s 151ms/step - loss: 19.8039 - mae: 3.2549 - val_loss: 31.7974 - val_mae: 4.8461\n\nEpoch 00001: val_mae improved from inf to 4.84613, saving model to trained_model/best_model\nEpoch 2/300\n115/115 [==============================] - 15s 127ms/step - loss: 2.5752 - mae: 1.1196 - val_loss: 55.9376 - val_mae: 6.0131\n\nEpoch 00006: val_mae did not improve from 3.77322\nEpoch 7/300\n115/115 [==============================] - 15s 127ms/step - loss: 2.0568 - mae: 0.9805 - val_loss: 29.0986 - val_mae: 4.0764\n\nEpoch 00007: val_mae did not improve from 3.77322\nEpoch 8/300\n115/115 [==============================] - 15s 128ms/step - loss: 1.6904 - mae: 0.9183 - val_loss: 69.4517 - val_mae: 6.8853\n\nEpoch 00008: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n\nEpoch 00008: val_mae did not improve from 3.77322\nEpoch 9/300\n115/115 [==============================] - 14s 126ms/step - loss: 1.4884 - mae: 0.8499 - val_loss: 60.2494 - val_mae: 6.2985\n\nEpoch 00009: val_mae did not improve from 3.77322\nEpoch 10/300\n115/115 [==============================] - 15s 128ms/step - loss: 1.2047 - mae: 0.7603 - val_loss: 28.1075 - val_mae: 3.9659\n\nEpoch 00010: val_mae did not improve from 3.77322\nEpoch 11/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.8753 - mae: 0.6534 - val_loss: 28.9715 - val_mae: 4.0711\n\nEpoch 00011: val_mae did not improve from 3.77322\nEpoch 12/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.7192 - mae: 0.5983 - val_loss: 31.0416 - val_mae: 4.1321\n\nEpoch 00012: val_mae did not improve from 3.77322\nEpoch 13/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.5826 - mae: 0.5340 - val_loss: 25.8883 - val_mae: 3.7470\n\nEpoch 00013: val_mae improved from 3.77322 to 3.74696, saving model to trained_model/best_model\nEpoch 14/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.5714 - mae: 0.5220 - val_loss: 26.9724 - val_mae: 3.8955\n\nEpoch 00014: val_mae did not improve from 3.74696\nEpoch 15/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.8866 - mae: 0.5897 - val_loss: 26.0329 - val_mae: 4.1675\n\nEpoch 00015: val_mae did not improve from 3.74696\nEpoch 16/300\n115/115 [==============================] - 14s 125ms/step - loss: 1.6284 - mae: 0.8055 - val_loss: 37.8362 - val_mae: 4.7536\n\nEpoch 00016: val_mae did not improve from 3.74696\nEpoch 17/300\n115/115 [==============================] - 15s 127ms/step - loss: 2.0150 - mae: 0.8530 - val_loss: 34.9227 - val_mae: 4.5467\n\nEpoch 00017: val_mae did not improve from 3.74696\nEpoch 18/300\n115/115 [==============================] - 14s 126ms/step - loss: 2.0330 - mae: 0.8883 - val_loss: 24.2693 - val_mae: 3.7615\n\nEpoch 00018: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n\nEpoch 00018: val_mae did not improve from 3.74696\nEpoch 19/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.9504 - mae: 0.6311 - val_loss: 25.3315 - val_mae: 3.7715\n\nEpoch 00019: val_mae did not improve from 3.74696\nEpoch 20/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.9291 - mae: 0.6011 - val_loss: 18.7076 - val_mae: 3.0328\n\nEpoch 00020: val_mae improved from 3.74696 to 3.03280, saving model to trained_model/best_model\nEpoch 21/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.7544 - mae: 0.5429 - val_loss: 20.2804 - val_mae: 3.3826\n\nEpoch 00021: val_mae did not improve from 3.03280\nEpoch 22/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.4841 - mae: 0.4645 - val_loss: 28.6407 - val_mae: 3.9527\n\nEpoch 00022: val_mae did not improve from 3.03280\nEpoch 23/300\n115/115 [==============================] - 15s 130ms/step - loss: 0.4959 - mae: 0.4592 - val_loss: 26.1505 - val_mae: 3.7852\n\nEpoch 00023: val_mae did not improve from 3.03280\nEpoch 24/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.3915 - mae: 0.4188 - val_loss: 10.2749 - val_mae: 2.1136\n\nEpoch 00024: val_mae improved from 3.03280 to 2.11359, saving model to trained_model/best_model\nEpoch 25/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.3684 - mae: 0.4121 - val_loss: 53.1893 - val_mae: 5.8104\n\nEpoch 00025: val_mae did not improve from 2.11359\nEpoch 26/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.7696 - mae: 0.5163 - val_loss: 46.1808 - val_mae: 5.1972\n\nEpoch 00026: val_mae did not improve from 2.11359\nEpoch 27/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.5680 - mae: 0.4739 - val_loss: 25.1771 - val_mae: 3.7631\n\nEpoch 00027: val_mae did not improve from 2.11359\nEpoch 28/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.8279 - mae: 0.5443 - val_loss: 28.7366 - val_mae: 3.9335\n\nEpoch 00028: val_mae did not improve from 2.11359\nEpoch 29/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.5785 - mae: 0.4801 - val_loss: 25.7711 - val_mae: 3.7413\n\nEpoch 00029: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n\nEpoch 00029: val_mae did not improve from 2.11359\nEpoch 30/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.4292 - mae: 0.4244 - val_loss: 28.7969 - val_mae: 4.1953\n\nEpoch 00030: val_mae did not improve from 2.11359\nEpoch 31/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.2752 - mae: 0.3583 - val_loss: 22.3796 - val_mae: 3.4303\n\nEpoch 00031: val_mae did not improve from 2.11359\nEpoch 32/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.2472 - mae: 0.3353 - val_loss: 14.5365 - val_mae: 2.6134\n\nEpoch 00032: val_mae did not improve from 2.11359\nEpoch 33/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.2330 - mae: 0.3336 - val_loss: 26.6221 - val_mae: 3.7563\n\nEpoch 00033: val_mae did not improve from 2.11359\nEpoch 34/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.1954 - mae: 0.3108 - val_loss: 5.2601 - val_mae: 1.2605\n\nEpoch 00034: val_mae improved from 2.11359 to 1.26051, saving model to trained_model/best_model\nEpoch 35/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.3444 - mae: 0.3531 - val_loss: 26.2330 - val_mae: 3.7361\n\nEpoch 00035: val_mae did not improve from 1.26051\nEpoch 36/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.2998 - mae: 0.3477 - val_loss: 25.0173 - val_mae: 3.7237\n\nEpoch 00036: val_mae did not improve from 1.26051\nEpoch 37/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.2536 - mae: 0.3369 - val_loss: 24.9422 - val_mae: 3.7878\n\nEpoch 00037: val_mae did not improve from 1.26051\nEpoch 38/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.2426 - mae: 0.3253 - val_loss: 25.3116 - val_mae: 3.6955\n\nEpoch 00038: val_mae did not improve from 1.26051\nEpoch 39/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.2005 - mae: 0.3109 - val_loss: 11.4680 - val_mae: 2.2757\n\nEpoch 00039: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n\nEpoch 00039: val_mae did not improve from 1.26051\nEpoch 40/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.1806 - mae: 0.2974 - val_loss: 20.6628 - val_mae: 3.2073\n\nEpoch 00040: val_mae did not improve from 1.26051\nEpoch 41/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.1568 - mae: 0.2787 - val_loss: 21.3507 - val_mae: 3.3033\n\nEpoch 00041: val_mae did not improve from 1.26051\nEpoch 42/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.1712 - mae: 0.2874 - val_loss: 26.0252 - val_mae: 3.7242\n\nEpoch 00042: val_mae did not improve from 1.26051\nEpoch 43/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.1724 - mae: 0.2842 - val_loss: 26.1662 - val_mae: 3.7182\n\nEpoch 00043: val_mae did not improve from 1.26051\nEpoch 44/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.2274 - mae: 0.3013 - val_loss: 25.7158 - val_mae: 3.7935\n\nEpoch 00044: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n\nEpoch 00044: val_mae did not improve from 1.26051\nEpoch 45/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.2953 - mae: 0.3309 - val_loss: 1080629.6250 - val_mae: 974.7697\n\nEpoch 00045: val_mae did not improve from 1.26051\nEpoch 46/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.4247 - mae: 0.3994 - val_loss: 2618.3533 - val_mae: 42.4081\n\nEpoch 00046: val_mae did not improve from 1.26051\nEpoch 47/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.6866 - mae: 0.4475 - val_loss: 39.6952 - val_mae: 4.8714\n\nEpoch 00047: val_mae did not improve from 1.26051\nEpoch 48/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.4704 - mae: 0.4072 - val_loss: 67.1668 - val_mae: 6.6505\n\nEpoch 00048: val_mae did not improve from 1.26051\nEpoch 49/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.4203 - mae: 0.3867 - val_loss: 26.6594 - val_mae: 3.7697\n\nEpoch 00049: ReduceLROnPlateau reducing learning rate to 0.0005314410547725857.\n\nEpoch 00049: val_mae did not improve from 1.26051\nEpoch 50/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.3356 - mae: 0.3601 - val_loss: 24.6772 - val_mae: 3.9190\n\nEpoch 00050: val_mae did not improve from 1.26051\nEpoch 51/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.2861 - mae: 0.3251 - val_loss: 26.3363 - val_mae: 3.7762\n\nEpoch 00051: val_mae did not improve from 1.26051\nEpoch 52/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.2607 - mae: 0.3180 - val_loss: 25.8744 - val_mae: 3.7471\n\nEpoch 00052: val_mae did not improve from 1.26051\nEpoch 53/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.2750 - mae: 0.3181 - val_loss: 26.5988 - val_mae: 3.7974\n\nEpoch 00053: val_mae did not improve from 1.26051\nEpoch 54/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.2571 - mae: 0.2964 - val_loss: 24.4334 - val_mae: 3.9789\n\nEpoch 00054: ReduceLROnPlateau reducing learning rate to 0.00047829695977270604.\n\nEpoch 00054: val_mae did not improve from 1.26051\nEpoch 55/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.2670 - mae: 0.3104 - val_loss: 25.2073 - val_mae: 3.9354\n\nEpoch 00055: val_mae did not improve from 1.26051\nEpoch 56/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.1493 - mae: 0.2688 - val_loss: 24.3671 - val_mae: 3.8751\n\nEpoch 00056: val_mae did not improve from 1.26051\nEpoch 57/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.1450 - mae: 0.2616 - val_loss: 6.9354 - val_mae: 1.3827\n\nEpoch 00057: val_mae did not improve from 1.26051\nEpoch 58/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.3234 - mae: 0.3089 - val_loss: 25.7287 - val_mae: 4.2575\n\nEpoch 00058: val_mae did not improve from 1.26051\nEpoch 59/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.3956 - mae: 0.3660 - val_loss: 21.9524 - val_mae: 3.4801\n\nEpoch 00059: ReduceLROnPlateau reducing learning rate to 0.0004304672533180565.\n\nEpoch 00059: val_mae did not improve from 1.26051\nEpoch 60/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.2446 - mae: 0.2988 - val_loss: 8.7442 - val_mae: 1.7228\n\nEpoch 00060: val_mae did not improve from 1.26051\nEpoch 61/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.1463 - mae: 0.2571 - val_loss: 25.0687 - val_mae: 3.7602\n\nEpoch 00061: val_mae did not improve from 1.26051\nEpoch 62/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.2615 - mae: 0.3031 - val_loss: 53.4034 - val_mae: 6.2734\n\nEpoch 00062: val_mae did not improve from 1.26051\nEpoch 63/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.1807 - mae: 0.2772 - val_loss: 48.0519 - val_mae: 5.9915\n\nEpoch 00063: val_mae did not improve from 1.26051\nEpoch 64/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.1424 - mae: 0.2487 - val_loss: 20.3544 - val_mae: 3.2212\n\nEpoch 00064: ReduceLROnPlateau reducing learning rate to 0.00038742052274756136.\n\nEpoch 00064: val_mae did not improve from 1.26051\nEpoch 65/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.1244 - mae: 0.2393 - val_loss: 4.8722 - val_mae: 0.9587\n\nEpoch 00065: val_mae improved from 1.26051 to 0.95866, saving model to trained_model/best_model\nEpoch 66/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.1186 - mae: 0.2352 - val_loss: 48.0792 - val_mae: 5.7916\n\nEpoch 00066: val_mae did not improve from 0.95866\nEpoch 67/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.1240 - mae: 0.2326 - val_loss: 26.6231 - val_mae: 3.7564\n\nEpoch 00067: val_mae did not improve from 0.95866\nEpoch 68/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.1977 - mae: 0.2616 - val_loss: 18.5643 - val_mae: 3.0441\n\nEpoch 00068: val_mae did not improve from 0.95866\nEpoch 69/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.1173 - mae: 0.2335 - val_loss: 18.1276 - val_mae: 2.9893\n\nEpoch 00069: val_mae did not improve from 0.95866\nEpoch 70/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.1672 - mae: 0.2457 - val_loss: 23.3538 - val_mae: 3.5692\n\nEpoch 00070: ReduceLROnPlateau reducing learning rate to 0.0003486784757114947.\n\nEpoch 00070: val_mae did not improve from 0.95866\nEpoch 71/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.1230 - mae: 0.2350 - val_loss: 39.4509 - val_mae: 5.4361\n\nEpoch 00071: val_mae did not improve from 0.95866\nEpoch 72/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.1019 - mae: 0.2227 - val_loss: 15.9302 - val_mae: 2.6702\n\nEpoch 00072: val_mae did not improve from 0.95866\nEpoch 73/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.1008 - mae: 0.2167 - val_loss: 17.5960 - val_mae: 2.8557\n\nEpoch 00073: val_mae did not improve from 0.95866\nEpoch 74/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.1035 - mae: 0.2185 - val_loss: 30.7497 - val_mae: 4.7224\n\nEpoch 00074: val_mae did not improve from 0.95866\nEpoch 75/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.1223 - mae: 0.2258 - val_loss: 19.2915 - val_mae: 3.1179\n\nEpoch 00075: ReduceLROnPlateau reducing learning rate to 0.00031381062290165574.\n\nEpoch 00075: val_mae did not improve from 0.95866\nEpoch 76/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0875 - mae: 0.2114 - val_loss: 6.5908 - val_mae: 1.4040\n\nEpoch 00076: val_mae did not improve from 0.95866\nEpoch 77/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0800 - mae: 0.2032 - val_loss: 10.2412 - val_mae: 1.9376\n\nEpoch 00077: val_mae did not improve from 0.95866\nEpoch 78/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0792 - mae: 0.2009 - val_loss: 14.8484 - val_mae: 2.4503\n\nEpoch 00078: val_mae did not improve from 0.95866\nEpoch 79/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0831 - mae: 0.2051 - val_loss: 4.8560 - val_mae: 0.9962\n\nEpoch 00079: val_mae did not improve from 0.95866\nEpoch 80/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0812 - mae: 0.2004 - val_loss: 18.1336 - val_mae: 2.8137\n\nEpoch 00080: ReduceLROnPlateau reducing learning rate to 0.0002824295632308349.\n\nEpoch 00080: val_mae did not improve from 0.95866\nEpoch 81/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.0911 - mae: 0.2090 - val_loss: 26.3350 - val_mae: 3.7412\n\nEpoch 00081: val_mae did not improve from 0.95866\nEpoch 82/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.1109 - mae: 0.2189 - val_loss: 38.9705 - val_mae: 5.3550\n\nEpoch 00082: val_mae did not improve from 0.95866\nEpoch 83/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.1039 - mae: 0.2119 - val_loss: 37.9505 - val_mae: 5.3094\n\nEpoch 00083: val_mae did not improve from 0.95866\nEpoch 84/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.1114 - mae: 0.2257 - val_loss: 15.1494 - val_mae: 2.5208\n\nEpoch 00084: val_mae did not improve from 0.95866\nEpoch 85/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0810 - mae: 0.2012 - val_loss: 20.1985 - val_mae: 3.1550\n\nEpoch 00085: ReduceLROnPlateau reducing learning rate to 0.00025418660952709616.\n\nEpoch 00085: val_mae did not improve from 0.95866\nEpoch 86/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0796 - mae: 0.1984 - val_loss: 22.4069 - val_mae: 3.4340\n\nEpoch 00086: val_mae did not improve from 0.95866\nEpoch 87/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.0684 - mae: 0.1896 - val_loss: 22.6470 - val_mae: 3.4135\n\nEpoch 00087: val_mae did not improve from 0.95866\nEpoch 88/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0782 - mae: 0.1912 - val_loss: 21.8266 - val_mae: 3.2863\n\nEpoch 00088: val_mae did not improve from 0.95866\nEpoch 89/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.1388 - mae: 0.2340 - val_loss: 20.7939 - val_mae: 3.2562\n\nEpoch 00089: val_mae did not improve from 0.95866\nEpoch 90/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.1016 - mae: 0.2114 - val_loss: 20.5688 - val_mae: 3.2530\n\nEpoch 00090: ReduceLROnPlateau reducing learning rate to 0.00022876793809700757.\n\nEpoch 00090: val_mae did not improve from 0.95866\nEpoch 91/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0886 - mae: 0.2078 - val_loss: 26.5281 - val_mae: 3.7449\n\nEpoch 00091: val_mae did not improve from 0.95866\nEpoch 92/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0833 - mae: 0.2058 - val_loss: 26.0096 - val_mae: 3.7096\n\nEpoch 00092: val_mae did not improve from 0.95866\nEpoch 93/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0671 - mae: 0.1868 - val_loss: 12.4345 - val_mae: 2.0100\n\nEpoch 00093: val_mae did not improve from 0.95866\nEpoch 94/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.0725 - mae: 0.1929 - val_loss: 6.0263 - val_mae: 1.1617\n\nEpoch 00094: val_mae did not improve from 0.95866\nEpoch 95/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0717 - mae: 0.1899 - val_loss: 16.1732 - val_mae: 2.7596\n\nEpoch 00095: ReduceLROnPlateau reducing learning rate to 0.00020589114428730683.\n\nEpoch 00095: val_mae did not improve from 0.95866\nEpoch 96/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0713 - mae: 0.1841 - val_loss: 26.2652 - val_mae: 3.7831\n\nEpoch 00096: val_mae did not improve from 0.95866\nEpoch 97/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.1206 - mae: 0.2161 - val_loss: 25.2242 - val_mae: 3.9081\n\nEpoch 00097: val_mae did not improve from 0.95866\nEpoch 98/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0914 - mae: 0.2060 - val_loss: 10.8483 - val_mae: 1.9545\n\nEpoch 00098: val_mae did not improve from 0.95866\nEpoch 99/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0866 - mae: 0.1972 - val_loss: 18.7766 - val_mae: 3.1287\n\nEpoch 00099: val_mae did not improve from 0.95866\nEpoch 100/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.1370 - mae: 0.2187 - val_loss: 50.0315 - val_mae: 5.4906\n\nEpoch 00100: ReduceLROnPlateau reducing learning rate to 0.00018530203378759326.\n\nEpoch 00100: val_mae did not improve from 0.95866\nEpoch 101/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0858 - mae: 0.2032 - val_loss: 25.6405 - val_mae: 3.7779\n\nEpoch 00101: val_mae did not improve from 0.95866\nEpoch 102/300\n115/115 [==============================] - 15s 129ms/step - loss: 0.0932 - mae: 0.2012 - val_loss: 26.6633 - val_mae: 3.7585\n\nEpoch 00102: val_mae did not improve from 0.95866\nEpoch 103/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0747 - mae: 0.1906 - val_loss: 8.5675 - val_mae: 1.7258\n\nEpoch 00103: val_mae did not improve from 0.95866\nEpoch 104/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0808 - mae: 0.1916 - val_loss: 13.2177 - val_mae: 2.2625\n\nEpoch 00104: val_mae did not improve from 0.95866\nEpoch 105/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0714 - mae: 0.1872 - val_loss: 12.0532 - val_mae: 2.0835\n\nEpoch 00105: ReduceLROnPlateau reducing learning rate to 0.00016677183302817866.\n\nEpoch 00105: val_mae did not improve from 0.95866\nEpoch 106/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0656 - mae: 0.1809 - val_loss: 21.8661 - val_mae: 3.3222\n\nEpoch 00106: val_mae did not improve from 0.95866\nEpoch 107/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.0614 - mae: 0.1780 - val_loss: 12.9008 - val_mae: 2.1525\n\nEpoch 00107: val_mae did not improve from 0.95866\nEpoch 108/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0644 - mae: 0.1799 - val_loss: 6.3711 - val_mae: 1.2541\n\nEpoch 00108: val_mae did not improve from 0.95866\nEpoch 109/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0638 - mae: 0.1812 - val_loss: 9.3812 - val_mae: 1.6749\n\nEpoch 00109: val_mae did not improve from 0.95866\nEpoch 110/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0753 - mae: 0.1871 - val_loss: 29.3536 - val_mae: 4.0638\n\nEpoch 00110: ReduceLROnPlateau reducing learning rate to 0.00015009464841568844.\n\nEpoch 00110: val_mae did not improve from 0.95866\nEpoch 111/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0807 - mae: 0.1911 - val_loss: 10.4250 - val_mae: 1.7567\n\nEpoch 00111: val_mae did not improve from 0.95866\nEpoch 112/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0611 - mae: 0.1775 - val_loss: 3.6629 - val_mae: 0.6941\n\nEpoch 00112: val_mae improved from 0.95866 to 0.69406, saving model to trained_model/best_model\nEpoch 113/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0640 - mae: 0.1814 - val_loss: 20.4483 - val_mae: 3.1285\n\nEpoch 00113: val_mae did not improve from 0.69406\nEpoch 114/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0567 - mae: 0.1726 - val_loss: 3.9461 - val_mae: 0.7177\n\nEpoch 00114: val_mae did not improve from 0.69406\nEpoch 115/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.0551 - mae: 0.1705 - val_loss: 9.1440 - val_mae: 1.5154\n\nEpoch 00115: val_mae did not improve from 0.69406\nEpoch 116/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0614 - mae: 0.1757 - val_loss: 10.2241 - val_mae: 1.5921\n\nEpoch 00116: val_mae did not improve from 0.69406\nEpoch 117/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0619 - mae: 0.1755 - val_loss: 15.5391 - val_mae: 2.3500\n\nEpoch 00117: ReduceLROnPlateau reducing learning rate to 0.0001350851875031367.\n\nEpoch 00117: val_mae did not improve from 0.69406\nEpoch 118/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0579 - mae: 0.1715 - val_loss: 9.0331 - val_mae: 1.5481\n\nEpoch 00118: val_mae did not improve from 0.69406\nEpoch 119/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0587 - mae: 0.1735 - val_loss: 8.0168 - val_mae: 1.4308\n\nEpoch 00119: val_mae did not improve from 0.69406\nEpoch 120/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0553 - mae: 0.1695 - val_loss: 3.3633 - val_mae: 0.5996\n\nEpoch 00120: val_mae improved from 0.69406 to 0.59956, saving model to trained_model/best_model\nEpoch 121/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0537 - mae: 0.1680 - val_loss: 3.7951 - val_mae: 0.6787\n\nEpoch 00121: val_mae did not improve from 0.59956\nEpoch 122/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0583 - mae: 0.1716 - val_loss: 7.2092 - val_mae: 1.3861\n\nEpoch 00122: val_mae did not improve from 0.59956\nEpoch 123/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0594 - mae: 0.1722 - val_loss: 18.2690 - val_mae: 2.9945\n\nEpoch 00123: val_mae did not improve from 0.59956\nEpoch 124/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.0548 - mae: 0.1687 - val_loss: 3.6966 - val_mae: 0.7059\n\nEpoch 00124: val_mae did not improve from 0.59956\nEpoch 125/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0599 - mae: 0.1697 - val_loss: 17.3141 - val_mae: 2.7919\n\nEpoch 00125: ReduceLROnPlateau reducing learning rate to 0.00012157666351413355.\n\nEpoch 00125: val_mae did not improve from 0.59956\nEpoch 126/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0637 - mae: 0.1723 - val_loss: 14.1234 - val_mae: 2.2099\n\nEpoch 00126: val_mae did not improve from 0.59956\nEpoch 127/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0551 - mae: 0.1682 - val_loss: 4.1523 - val_mae: 0.7435\n\nEpoch 00127: val_mae did not improve from 0.59956\nEpoch 128/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0573 - mae: 0.1695 - val_loss: 4.6847 - val_mae: 0.7766\n\nEpoch 00128: val_mae did not improve from 0.59956\nEpoch 129/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0575 - mae: 0.1706 - val_loss: 5.7362 - val_mae: 1.0610\n\nEpoch 00129: val_mae did not improve from 0.59956\nEpoch 130/300\n115/115 [==============================] - 15s 129ms/step - loss: 0.0570 - mae: 0.1664 - val_loss: 14.8646 - val_mae: 2.5356\n\nEpoch 00130: ReduceLROnPlateau reducing learning rate to 0.00010941899454337544.\n\nEpoch 00130: val_mae did not improve from 0.59956\nEpoch 131/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0543 - mae: 0.1677 - val_loss: 12.3035 - val_mae: 2.0901\n\nEpoch 00131: val_mae did not improve from 0.59956\nEpoch 132/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0552 - mae: 0.1691 - val_loss: 5.1621 - val_mae: 0.9750\n\nEpoch 00132: val_mae did not improve from 0.59956\nEpoch 133/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0533 - mae: 0.1655 - val_loss: 4.5286 - val_mae: 0.8505\n\nEpoch 00133: val_mae did not improve from 0.59956\nEpoch 134/300\n115/115 [==============================] - 14s 124ms/step - loss: 0.0608 - mae: 0.1721 - val_loss: 5.9678 - val_mae: 1.1983\n\nEpoch 00134: val_mae did not improve from 0.59956\nEpoch 135/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0554 - mae: 0.1644 - val_loss: 18.6363 - val_mae: 2.9421\n\nEpoch 00135: ReduceLROnPlateau reducing learning rate to 9.847709443420172e-05.\n\nEpoch 00135: val_mae did not improve from 0.59956\nEpoch 136/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0515 - mae: 0.1642 - val_loss: 11.2906 - val_mae: 1.9773\n\nEpoch 00136: val_mae did not improve from 0.59956\nEpoch 137/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0523 - mae: 0.1635 - val_loss: 3.2791 - val_mae: 0.5851\n\nEpoch 00137: val_mae improved from 0.59956 to 0.58505, saving model to trained_model/best_model\nEpoch 138/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0489 - mae: 0.1606 - val_loss: 5.2034 - val_mae: 0.8887\n\nEpoch 00138: val_mae did not improve from 0.58505\nEpoch 139/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0561 - mae: 0.1659 - val_loss: 8.7055 - val_mae: 1.4678\n\nEpoch 00139: val_mae did not improve from 0.58505\nEpoch 140/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0618 - mae: 0.1756 - val_loss: 20.0659 - val_mae: 3.1019\n\nEpoch 00140: val_mae did not improve from 0.58505\nEpoch 141/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0550 - mae: 0.1652 - val_loss: 7.4484 - val_mae: 1.3429\n\nEpoch 00141: val_mae did not improve from 0.58505\nEpoch 142/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0510 - mae: 0.1598 - val_loss: 13.7613 - val_mae: 2.3791\n\nEpoch 00142: ReduceLROnPlateau reducing learning rate to 8.862938630045391e-05.\n\nEpoch 00142: val_mae did not improve from 0.58505\nEpoch 143/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0501 - mae: 0.1613 - val_loss: 4.7315 - val_mae: 0.8964\n\nEpoch 00143: val_mae did not improve from 0.58505\nEpoch 144/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0511 - mae: 0.1601 - val_loss: 3.3248 - val_mae: 0.5743\n\nEpoch 00144: val_mae improved from 0.58505 to 0.57433, saving model to trained_model/best_model\nEpoch 145/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0498 - mae: 0.1583 - val_loss: 4.2537 - val_mae: 0.7530\n\nEpoch 00145: val_mae did not improve from 0.57433\nEpoch 146/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0516 - mae: 0.1617 - val_loss: 4.6492 - val_mae: 0.8743\n\nEpoch 00146: val_mae did not improve from 0.57433\nEpoch 147/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0531 - mae: 0.1642 - val_loss: 3.9031 - val_mae: 0.6699\n\nEpoch 00147: val_mae did not improve from 0.57433\nEpoch 148/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0530 - mae: 0.1646 - val_loss: 10.6283 - val_mae: 1.7219\n\nEpoch 00148: val_mae did not improve from 0.57433\nEpoch 149/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0508 - mae: 0.1590 - val_loss: 3.9262 - val_mae: 0.7173\n\nEpoch 00149: ReduceLROnPlateau reducing learning rate to 7.976644701557234e-05.\n\nEpoch 00149: val_mae did not improve from 0.57433\nEpoch 150/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.0508 - mae: 0.1611 - val_loss: 7.0735 - val_mae: 1.2107\n\nEpoch 00150: val_mae did not improve from 0.57433\nEpoch 151/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0508 - mae: 0.1604 - val_loss: 8.7635 - val_mae: 1.3904\n\nEpoch 00151: val_mae did not improve from 0.57433\nEpoch 152/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0489 - mae: 0.1593 - val_loss: 7.5109 - val_mae: 1.3023\n\nEpoch 00152: val_mae did not improve from 0.57433\nEpoch 153/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0470 - mae: 0.1571 - val_loss: 7.1470 - val_mae: 1.2197\n\nEpoch 00153: val_mae did not improve from 0.57433\nEpoch 154/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0496 - mae: 0.1583 - val_loss: 5.0617 - val_mae: 0.8999\n\nEpoch 00154: ReduceLROnPlateau reducing learning rate to 7.178980231401511e-05.\n\nEpoch 00154: val_mae did not improve from 0.57433\nEpoch 155/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0509 - mae: 0.1574 - val_loss: 18.8738 - val_mae: 2.8091\n\nEpoch 00155: val_mae did not improve from 0.57433\nEpoch 156/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0467 - mae: 0.1563 - val_loss: 7.2658 - val_mae: 1.2789\n\nEpoch 00156: val_mae did not improve from 0.57433\nEpoch 157/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0478 - mae: 0.1565 - val_loss: 3.6178 - val_mae: 0.6277\n\nEpoch 00157: val_mae did not improve from 0.57433\nEpoch 158/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0487 - mae: 0.1579 - val_loss: 3.2854 - val_mae: 0.6327\n\nEpoch 00158: val_mae did not improve from 0.57433\nEpoch 159/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0476 - mae: 0.1551 - val_loss: 3.2768 - val_mae: 0.5487\n\nEpoch 00159: val_mae improved from 0.57433 to 0.54867, saving model to trained_model/best_model\nEpoch 160/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0473 - mae: 0.1551 - val_loss: 4.9284 - val_mae: 0.8584\n\nEpoch 00160: val_mae did not improve from 0.54867\nEpoch 161/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0455 - mae: 0.1512 - val_loss: 10.3544 - val_mae: 1.6516\n\nEpoch 00161: val_mae did not improve from 0.54867\nEpoch 162/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0470 - mae: 0.1563 - val_loss: 4.5380 - val_mae: 0.7729\n\nEpoch 00162: val_mae did not improve from 0.54867\nEpoch 163/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0466 - mae: 0.1545 - val_loss: 4.1814 - val_mae: 0.7013\n\nEpoch 00163: val_mae did not improve from 0.54867\nEpoch 164/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0658 - mae: 0.1606 - val_loss: 16.1526 - val_mae: 2.6503\n\nEpoch 00164: ReduceLROnPlateau reducing learning rate to 6.461082011810504e-05.\n\nEpoch 00164: val_mae did not improve from 0.54867\nEpoch 165/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0474 - mae: 0.1555 - val_loss: 3.4736 - val_mae: 0.6208\n\nEpoch 00165: val_mae did not improve from 0.54867\nEpoch 166/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0512 - mae: 0.1576 - val_loss: 12.1893 - val_mae: 2.0274\n\nEpoch 00166: val_mae did not improve from 0.54867\nEpoch 167/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.0471 - mae: 0.1527 - val_loss: 11.2342 - val_mae: 1.9675\n\nEpoch 00167: val_mae did not improve from 0.54867\nEpoch 168/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0480 - mae: 0.1566 - val_loss: 16.2209 - val_mae: 2.6020\n\nEpoch 00168: val_mae did not improve from 0.54867\nEpoch 169/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0479 - mae: 0.1549 - val_loss: 3.6598 - val_mae: 0.6652\n\nEpoch 00169: ReduceLROnPlateau reducing learning rate to 5.8149741380475466e-05.\n\nEpoch 00169: val_mae did not improve from 0.54867\nEpoch 170/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0481 - mae: 0.1567 - val_loss: 3.7998 - val_mae: 0.6785\n\nEpoch 00170: val_mae did not improve from 0.54867\nEpoch 171/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.0449 - mae: 0.1507 - val_loss: 3.0927 - val_mae: 0.5401\n\nEpoch 00171: val_mae improved from 0.54867 to 0.54013, saving model to trained_model/best_model\nEpoch 172/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0452 - mae: 0.1521 - val_loss: 2.8935 - val_mae: 0.4874\n\nEpoch 00172: val_mae improved from 0.54013 to 0.48736, saving model to trained_model/best_model\nEpoch 173/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.0482 - mae: 0.1530 - val_loss: 3.8710 - val_mae: 0.6649\n\nEpoch 00173: val_mae did not improve from 0.48736\nEpoch 174/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0452 - mae: 0.1526 - val_loss: 3.0140 - val_mae: 0.5079\n\nEpoch 00174: val_mae did not improve from 0.48736\nEpoch 175/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0434 - mae: 0.1506 - val_loss: 3.6701 - val_mae: 0.6418\n\nEpoch 00175: val_mae did not improve from 0.48736\nEpoch 176/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0485 - mae: 0.1536 - val_loss: 10.3949 - val_mae: 1.7638\n\nEpoch 00176: val_mae did not improve from 0.48736\nEpoch 177/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0450 - mae: 0.1530 - val_loss: 6.6840 - val_mae: 1.2372\n\nEpoch 00177: ReduceLROnPlateau reducing learning rate to 5.233476658759173e-05.\n\nEpoch 00177: val_mae did not improve from 0.48736\nEpoch 178/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0458 - mae: 0.1530 - val_loss: 3.7364 - val_mae: 0.6607\n\nEpoch 00178: val_mae did not improve from 0.48736\nEpoch 179/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0450 - mae: 0.1521 - val_loss: 6.5086 - val_mae: 1.1129\n\nEpoch 00179: val_mae did not improve from 0.48736\nEpoch 180/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0479 - mae: 0.1524 - val_loss: 3.6696 - val_mae: 0.6268\n\nEpoch 00180: val_mae did not improve from 0.48736\nEpoch 181/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0447 - mae: 0.1512 - val_loss: 6.4619 - val_mae: 1.1175\n\nEpoch 00181: val_mae did not improve from 0.48736\nEpoch 182/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0438 - mae: 0.1486 - val_loss: 3.3162 - val_mae: 0.5283\n\nEpoch 00182: ReduceLROnPlateau reducing learning rate to 4.7101289601414466e-05.\n\nEpoch 00182: val_mae did not improve from 0.48736\nEpoch 183/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0450 - mae: 0.1507 - val_loss: 4.0649 - val_mae: 0.7045\n\nEpoch 00183: val_mae did not improve from 0.48736\nEpoch 184/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.0472 - mae: 0.1497 - val_loss: 3.4338 - val_mae: 0.5898\n\nEpoch 00184: val_mae did not improve from 0.48736\nEpoch 185/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0462 - mae: 0.1528 - val_loss: 4.9789 - val_mae: 0.9732\n\nEpoch 00185: val_mae did not improve from 0.48736\nEpoch 186/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0463 - mae: 0.1540 - val_loss: 3.4134 - val_mae: 0.5506\n\nEpoch 00186: val_mae did not improve from 0.48736\nEpoch 187/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0437 - mae: 0.1503 - val_loss: 3.3869 - val_mae: 0.5349\n\nEpoch 00187: ReduceLROnPlateau reducing learning rate to 4.239116096869111e-05.\n\nEpoch 00187: val_mae did not improve from 0.48736\nEpoch 188/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0428 - mae: 0.1481 - val_loss: 3.4295 - val_mae: 0.5412\n\nEpoch 00188: val_mae did not improve from 0.48736\nEpoch 189/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0441 - mae: 0.1480 - val_loss: 22.4089 - val_mae: 3.3534\n\nEpoch 00189: val_mae did not improve from 0.48736\nEpoch 190/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0438 - mae: 0.1498 - val_loss: 7.7801 - val_mae: 1.3707\n\nEpoch 00190: val_mae did not improve from 0.48736\nEpoch 191/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0438 - mae: 0.1492 - val_loss: 3.2212 - val_mae: 0.5011\n\nEpoch 00191: val_mae did not improve from 0.48736\nEpoch 192/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0534 - mae: 0.1481 - val_loss: 4.5809 - val_mae: 0.7872\n\nEpoch 00192: ReduceLROnPlateau reducing learning rate to 3.815204618149437e-05.\n\nEpoch 00192: val_mae did not improve from 0.48736\nEpoch 193/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0489 - mae: 0.1619 - val_loss: 17.9905 - val_mae: 2.6650\n\nEpoch 00193: val_mae did not improve from 0.48736\nEpoch 194/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0476 - mae: 0.1537 - val_loss: 3.7260 - val_mae: 0.6069\n\nEpoch 00194: val_mae did not improve from 0.48736\nEpoch 195/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.0428 - mae: 0.1479 - val_loss: 3.1206 - val_mae: 0.5076\n\nEpoch 00195: val_mae did not improve from 0.48736\nEpoch 196/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0441 - mae: 0.1474 - val_loss: 3.1245 - val_mae: 0.4987\n\nEpoch 00196: val_mae did not improve from 0.48736\nEpoch 197/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0431 - mae: 0.1473 - val_loss: 3.2034 - val_mae: 0.4941\n\nEpoch 00197: ReduceLROnPlateau reducing learning rate to 3.4336842873017304e-05.\n\nEpoch 00197: val_mae did not improve from 0.48736\nEpoch 198/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0437 - mae: 0.1449 - val_loss: 3.4352 - val_mae: 0.5819\n\nEpoch 00198: val_mae did not improve from 0.48736\nEpoch 199/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0431 - mae: 0.1485 - val_loss: 3.1012 - val_mae: 0.5142\n\nEpoch 00199: val_mae did not improve from 0.48736\nEpoch 200/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0451 - mae: 0.1514 - val_loss: 3.1622 - val_mae: 0.5032\n\nEpoch 00200: val_mae did not improve from 0.48736\nEpoch 201/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0433 - mae: 0.1480 - val_loss: 3.2452 - val_mae: 0.5274\n\nEpoch 00201: val_mae did not improve from 0.48736\nEpoch 202/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0421 - mae: 0.1471 - val_loss: 3.1143 - val_mae: 0.4827\n\nEpoch 00202: val_mae improved from 0.48736 to 0.48267, saving model to trained_model/best_model\nEpoch 203/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0422 - mae: 0.1460 - val_loss: 3.1409 - val_mae: 0.4951\n\nEpoch 00203: val_mae did not improve from 0.48267\nEpoch 204/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0420 - mae: 0.1467 - val_loss: 3.2170 - val_mae: 0.5179\n\nEpoch 00204: val_mae did not improve from 0.48267\nEpoch 205/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0419 - mae: 0.1433 - val_loss: 3.1452 - val_mae: 0.5013\n\nEpoch 00205: val_mae did not improve from 0.48267\nEpoch 206/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0507 - mae: 0.1507 - val_loss: 6.6741 - val_mae: 1.1632\n\nEpoch 00206: val_mae did not improve from 0.48267\nEpoch 207/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0433 - mae: 0.1461 - val_loss: 3.8628 - val_mae: 0.6212\n\nEpoch 00207: ReduceLROnPlateau reducing learning rate to 3.0903160222806036e-05.\n\nEpoch 00207: val_mae did not improve from 0.48267\nEpoch 208/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0439 - mae: 0.1465 - val_loss: 3.5345 - val_mae: 0.5636\n\nEpoch 00208: val_mae did not improve from 0.48267\nEpoch 209/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0443 - mae: 0.1494 - val_loss: 4.4156 - val_mae: 0.7219\n\nEpoch 00209: val_mae did not improve from 0.48267\nEpoch 210/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0433 - mae: 0.1470 - val_loss: 3.1397 - val_mae: 0.5143\n\nEpoch 00210: val_mae did not improve from 0.48267\nEpoch 211/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0415 - mae: 0.1439 - val_loss: 3.2579 - val_mae: 0.5242\n\nEpoch 00211: val_mae did not improve from 0.48267\nEpoch 212/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.0425 - mae: 0.1460 - val_loss: 3.4779 - val_mae: 0.5796\n\nEpoch 00212: ReduceLROnPlateau reducing learning rate to 2.7812844200525434e-05.\n\nEpoch 00212: val_mae did not improve from 0.48267\nEpoch 213/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0427 - mae: 0.1482 - val_loss: 3.3182 - val_mae: 0.5435\n\nEpoch 00213: val_mae did not improve from 0.48267\nEpoch 214/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0422 - mae: 0.1452 - val_loss: 3.3226 - val_mae: 0.5415\n\nEpoch 00214: val_mae did not improve from 0.48267\nEpoch 215/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0430 - mae: 0.1453 - val_loss: 3.2158 - val_mae: 0.5165\n\nEpoch 00215: val_mae did not improve from 0.48267\nEpoch 216/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0422 - mae: 0.1450 - val_loss: 3.7101 - val_mae: 0.6514\n\nEpoch 00216: val_mae did not improve from 0.48267\nEpoch 217/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0436 - mae: 0.1481 - val_loss: 3.1550 - val_mae: 0.5173\n\nEpoch 00217: ReduceLROnPlateau reducing learning rate to 2.5031560107890984e-05.\n\nEpoch 00217: val_mae did not improve from 0.48267\nEpoch 218/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0469 - mae: 0.1492 - val_loss: 4.3234 - val_mae: 0.7186\n\nEpoch 00218: val_mae did not improve from 0.48267\nEpoch 219/300\n115/115 [==============================] - 14s 124ms/step - loss: 0.0426 - mae: 0.1458 - val_loss: 3.7772 - val_mae: 0.6119\n\nEpoch 00219: val_mae did not improve from 0.48267\nEpoch 220/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0408 - mae: 0.1432 - val_loss: 3.0669 - val_mae: 0.4769\n\nEpoch 00220: val_mae improved from 0.48267 to 0.47687, saving model to trained_model/best_model\nEpoch 221/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0424 - mae: 0.1462 - val_loss: 3.0312 - val_mae: 0.4776\n\nEpoch 00221: val_mae did not improve from 0.47687\nEpoch 222/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0435 - mae: 0.1474 - val_loss: 3.4162 - val_mae: 0.5478\n\nEpoch 00222: val_mae did not improve from 0.47687\nEpoch 223/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0411 - mae: 0.1436 - val_loss: 3.0227 - val_mae: 0.4920\n\nEpoch 00223: val_mae did not improve from 0.47687\nEpoch 224/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0426 - mae: 0.1471 - val_loss: 3.0304 - val_mae: 0.4726\n\nEpoch 00224: val_mae improved from 0.47687 to 0.47263, saving model to trained_model/best_model\nEpoch 225/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0405 - mae: 0.1444 - val_loss: 3.0186 - val_mae: 0.4770\n\nEpoch 00225: val_mae did not improve from 0.47263\nEpoch 226/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0410 - mae: 0.1449 - val_loss: 3.4951 - val_mae: 0.5748\n\nEpoch 00226: val_mae did not improve from 0.47263\nEpoch 227/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0423 - mae: 0.1448 - val_loss: 3.5049 - val_mae: 0.5815\n\nEpoch 00227: val_mae did not improve from 0.47263\nEpoch 228/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0398 - mae: 0.1427 - val_loss: 3.0401 - val_mae: 0.4844\n\nEpoch 00228: val_mae did not improve from 0.47263\nEpoch 229/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0432 - mae: 0.1452 - val_loss: 3.0529 - val_mae: 0.4966\n\nEpoch 00229: ReduceLROnPlateau reducing learning rate to 2.2528404588229024e-05.\n\nEpoch 00229: val_mae did not improve from 0.47263\nEpoch 230/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0417 - mae: 0.1455 - val_loss: 3.0974 - val_mae: 0.4980\n\nEpoch 00230: val_mae did not improve from 0.47263\nEpoch 231/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0409 - mae: 0.1456 - val_loss: 3.1192 - val_mae: 0.4938\n\nEpoch 00231: val_mae did not improve from 0.47263\nEpoch 232/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0394 - mae: 0.1434 - val_loss: 3.1086 - val_mae: 0.4860\n\nEpoch 00232: val_mae did not improve from 0.47263\nEpoch 233/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0416 - mae: 0.1446 - val_loss: 3.0479 - val_mae: 0.4766\n\nEpoch 00233: val_mae did not improve from 0.47263\nEpoch 234/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0418 - mae: 0.1452 - val_loss: 3.1676 - val_mae: 0.5115\n\nEpoch 00234: ReduceLROnPlateau reducing learning rate to 2.0275563474569936e-05.\n\nEpoch 00234: val_mae did not improve from 0.47263\nEpoch 235/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0418 - mae: 0.1436 - val_loss: 9.2968 - val_mae: 1.5260\n\nEpoch 00235: val_mae did not improve from 0.47263\nEpoch 236/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0404 - mae: 0.1427 - val_loss: 4.0406 - val_mae: 0.6559\n\nEpoch 00236: val_mae did not improve from 0.47263\nEpoch 237/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0406 - mae: 0.1428 - val_loss: 3.0122 - val_mae: 0.4803\n\nEpoch 00237: val_mae did not improve from 0.47263\nEpoch 238/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0459 - mae: 0.1460 - val_loss: 6.2784 - val_mae: 1.1114\n\nEpoch 00238: val_mae did not improve from 0.47263\nEpoch 239/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0400 - mae: 0.1426 - val_loss: 3.2558 - val_mae: 0.5331\n\nEpoch 00239: ReduceLROnPlateau reducing learning rate to 1.8248007290821987e-05.\n\nEpoch 00239: val_mae did not improve from 0.47263\nEpoch 240/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0402 - mae: 0.1431 - val_loss: 3.1488 - val_mae: 0.5015\n\nEpoch 00240: val_mae did not improve from 0.47263\nEpoch 241/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0417 - mae: 0.1460 - val_loss: 3.1639 - val_mae: 0.5188\n\nEpoch 00241: val_mae did not improve from 0.47263\nEpoch 242/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0433 - mae: 0.1448 - val_loss: 3.1151 - val_mae: 0.5185\n\nEpoch 00242: val_mae did not improve from 0.47263\nEpoch 243/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0402 - mae: 0.1430 - val_loss: 3.0824 - val_mae: 0.5035\n\nEpoch 00243: val_mae did not improve from 0.47263\nEpoch 244/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0421 - mae: 0.1451 - val_loss: 3.0006 - val_mae: 0.4762\n\nEpoch 00244: ReduceLROnPlateau reducing learning rate to 1.6423206398030745e-05.\n\nEpoch 00244: val_mae did not improve from 0.47263\nEpoch 245/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0419 - mae: 0.1445 - val_loss: 3.0567 - val_mae: 0.4697\n\nEpoch 00245: val_mae improved from 0.47263 to 0.46972, saving model to trained_model/best_model\nEpoch 246/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0408 - mae: 0.1435 - val_loss: 3.0020 - val_mae: 0.4763\n\nEpoch 00246: val_mae did not improve from 0.46972\nEpoch 247/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0418 - mae: 0.1423 - val_loss: 3.6383 - val_mae: 0.5893\n\nEpoch 00247: val_mae did not improve from 0.46972\nEpoch 248/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0425 - mae: 0.1458 - val_loss: 3.1883 - val_mae: 0.4965\n\nEpoch 00248: val_mae did not improve from 0.46972\nEpoch 249/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0397 - mae: 0.1419 - val_loss: 3.1146 - val_mae: 0.4846\n\nEpoch 00249: val_mae did not improve from 0.46972\nEpoch 250/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0400 - mae: 0.1429 - val_loss: 3.2064 - val_mae: 0.5018\n\nEpoch 00250: ReduceLROnPlateau reducing learning rate to 1.4780885430809576e-05.\n\nEpoch 00250: val_mae did not improve from 0.46972\nEpoch 251/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0402 - mae: 0.1427 - val_loss: 3.2328 - val_mae: 0.5126\n\nEpoch 00251: val_mae did not improve from 0.46972\nEpoch 252/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0401 - mae: 0.1430 - val_loss: 3.2526 - val_mae: 0.5276\n\nEpoch 00252: val_mae did not improve from 0.46972\nEpoch 253/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0426 - mae: 0.1425 - val_loss: 3.1218 - val_mae: 0.4930\n\nEpoch 00253: val_mae did not improve from 0.46972\nEpoch 254/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0396 - mae: 0.1418 - val_loss: 3.3188 - val_mae: 0.5686\n\nEpoch 00254: val_mae did not improve from 0.46972\nEpoch 255/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0434 - mae: 0.1444 - val_loss: 3.8955 - val_mae: 0.6553\n\nEpoch 00255: ReduceLROnPlateau reducing learning rate to 1.3302796560310526e-05.\n\nEpoch 00255: val_mae did not improve from 0.46972\nEpoch 256/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0419 - mae: 0.1450 - val_loss: 3.0300 - val_mae: 0.4732\n\nEpoch 00256: val_mae did not improve from 0.46972\nEpoch 257/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0401 - mae: 0.1431 - val_loss: 3.0372 - val_mae: 0.4700\n\nEpoch 00257: val_mae did not improve from 0.46972\nEpoch 258/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0426 - mae: 0.1429 - val_loss: 3.1101 - val_mae: 0.4811\n\nEpoch 00258: val_mae did not improve from 0.46972\nEpoch 259/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0399 - mae: 0.1414 - val_loss: 2.9872 - val_mae: 0.4802\n\nEpoch 00259: val_mae did not improve from 0.46972\nEpoch 260/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0407 - mae: 0.1443 - val_loss: 3.0011 - val_mae: 0.4702\n\nEpoch 00260: ReduceLROnPlateau reducing learning rate to 1.1972517313552089e-05.\n\nEpoch 00260: val_mae did not improve from 0.46972\nEpoch 261/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0397 - mae: 0.1423 - val_loss: 3.0734 - val_mae: 0.4753\n\nEpoch 00261: val_mae did not improve from 0.46972\nEpoch 262/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0402 - mae: 0.1429 - val_loss: 3.1185 - val_mae: 0.4772\n\nEpoch 00262: val_mae did not improve from 0.46972\nEpoch 263/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0405 - mae: 0.1426 - val_loss: 3.0767 - val_mae: 0.4752\n\nEpoch 00263: val_mae did not improve from 0.46972\nEpoch 264/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0397 - mae: 0.1411 - val_loss: 3.0837 - val_mae: 0.4738\n\nEpoch 00264: val_mae did not improve from 0.46972\nEpoch 265/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0400 - mae: 0.1424 - val_loss: 3.0792 - val_mae: 0.4795\n\nEpoch 00265: ReduceLROnPlateau reducing learning rate to 1.077526558219688e-05.\n\nEpoch 00265: val_mae did not improve from 0.46972\nEpoch 266/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0401 - mae: 0.1418 - val_loss: 3.1318 - val_mae: 0.4837\n\nEpoch 00266: val_mae did not improve from 0.46972\nEpoch 267/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0391 - mae: 0.1417 - val_loss: 3.1395 - val_mae: 0.4905\n\nEpoch 00267: val_mae did not improve from 0.46972\nEpoch 268/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0383 - mae: 0.1385 - val_loss: 3.1578 - val_mae: 0.4955\n\nEpoch 00268: val_mae did not improve from 0.46972\nEpoch 269/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0402 - mae: 0.1427 - val_loss: 3.0991 - val_mae: 0.4857\n\nEpoch 00269: val_mae did not improve from 0.46972\nEpoch 270/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0407 - mae: 0.1434 - val_loss: 3.1499 - val_mae: 0.4961\n\nEpoch 00270: ReduceLROnPlateau reducing learning rate to 9.697739187686238e-06.\n\nEpoch 00270: val_mae did not improve from 0.46972\nEpoch 271/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0432 - mae: 0.1445 - val_loss: 3.3210 - val_mae: 0.5358\n\nEpoch 00271: val_mae did not improve from 0.46972\nEpoch 272/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0419 - mae: 0.1437 - val_loss: 3.2814 - val_mae: 0.5362\n\nEpoch 00272: val_mae did not improve from 0.46972\nEpoch 273/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0420 - mae: 0.1436 - val_loss: 3.2672 - val_mae: 0.5101\n\nEpoch 00273: val_mae did not improve from 0.46972\nEpoch 274/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0412 - mae: 0.1422 - val_loss: 3.1858 - val_mae: 0.4941\n\nEpoch 00274: val_mae did not improve from 0.46972\nEpoch 275/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0409 - mae: 0.1432 - val_loss: 3.1977 - val_mae: 0.4868\n\nEpoch 00275: ReduceLROnPlateau reducing learning rate to 8.727965268917615e-06.\n\nEpoch 00275: val_mae did not improve from 0.46972\nEpoch 276/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0399 - mae: 0.1423 - val_loss: 3.1481 - val_mae: 0.4790\n\nEpoch 00276: val_mae did not improve from 0.46972\nEpoch 277/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0395 - mae: 0.1396 - val_loss: 3.1827 - val_mae: 0.4865\n\nEpoch 00277: val_mae did not improve from 0.46972\nEpoch 278/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0405 - mae: 0.1416 - val_loss: 3.1876 - val_mae: 0.5002\n\nEpoch 00278: val_mae did not improve from 0.46972\nEpoch 279/300\n115/115 [==============================] - 15s 128ms/step - loss: 0.0417 - mae: 0.1446 - val_loss: 3.1953 - val_mae: 0.4971\n\nEpoch 00279: val_mae did not improve from 0.46972\nEpoch 280/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0400 - mae: 0.1403 - val_loss: 3.1224 - val_mae: 0.4784\n\nEpoch 00280: ReduceLROnPlateau reducing learning rate to 7.855168496462283e-06.\n\nEpoch 00280: val_mae did not improve from 0.46972\nEpoch 281/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0403 - mae: 0.1420 - val_loss: 3.0292 - val_mae: 0.5067\n\nEpoch 00281: val_mae did not improve from 0.46972\nEpoch 282/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0386 - mae: 0.1397 - val_loss: 3.1025 - val_mae: 0.4735\n\nEpoch 00282: val_mae did not improve from 0.46972\nEpoch 283/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0397 - mae: 0.1401 - val_loss: 3.1425 - val_mae: 0.4827\n\nEpoch 00283: val_mae did not improve from 0.46972\nEpoch 284/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0416 - mae: 0.1428 - val_loss: 3.2133 - val_mae: 0.4969\n\nEpoch 00284: val_mae did not improve from 0.46972\nEpoch 285/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0408 - mae: 0.1426 - val_loss: 3.1716 - val_mae: 0.4839\n\nEpoch 00285: ReduceLROnPlateau reducing learning rate to 7.069651564961533e-06.\n\nEpoch 00285: val_mae did not improve from 0.46972\nEpoch 286/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0387 - mae: 0.1389 - val_loss: 3.0551 - val_mae: 0.4741\n\nEpoch 00286: val_mae did not improve from 0.46972\nEpoch 287/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0398 - mae: 0.1408 - val_loss: 3.1334 - val_mae: 0.4819\n\nEpoch 00287: val_mae did not improve from 0.46972\nEpoch 288/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0388 - mae: 0.1407 - val_loss: 3.1393 - val_mae: 0.4794\n\nEpoch 00288: val_mae did not improve from 0.46972\nEpoch 289/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0404 - mae: 0.1431 - val_loss: 3.1490 - val_mae: 0.4798\n\nEpoch 00289: val_mae did not improve from 0.46972\nEpoch 290/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0565 - mae: 0.1434 - val_loss: 3.3541 - val_mae: 0.5518\n\nEpoch 00290: ReduceLROnPlateau reducing learning rate to 6.362686326610856e-06.\n\nEpoch 00290: val_mae did not improve from 0.46972\nEpoch 291/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0405 - mae: 0.1418 - val_loss: 3.1717 - val_mae: 0.4967\n\nEpoch 00291: val_mae did not improve from 0.46972\nEpoch 292/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0395 - mae: 0.1413 - val_loss: 3.1886 - val_mae: 0.4932\n\nEpoch 00292: val_mae did not improve from 0.46972\nEpoch 293/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0393 - mae: 0.1404 - val_loss: 3.1742 - val_mae: 0.4836\n\nEpoch 00293: val_mae did not improve from 0.46972\nEpoch 294/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0384 - mae: 0.1385 - val_loss: 3.1879 - val_mae: 0.4896\n\nEpoch 00294: val_mae did not improve from 0.46972\nEpoch 295/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0402 - mae: 0.1437 - val_loss: 3.1817 - val_mae: 0.4887\n\nEpoch 00295: ReduceLROnPlateau reducing learning rate to 5.726417612095247e-06.\n\nEpoch 00295: val_mae did not improve from 0.46972\nEpoch 296/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0391 - mae: 0.1409 - val_loss: 3.1919 - val_mae: 0.4925\n\nEpoch 00296: val_mae did not improve from 0.46972\nEpoch 297/300\n115/115 [==============================] - 14s 126ms/step - loss: 0.0409 - mae: 0.1429 - val_loss: 3.1923 - val_mae: 0.4855\n\nEpoch 00297: val_mae did not improve from 0.46972\nEpoch 298/300\n115/115 [==============================] - 15s 126ms/step - loss: 0.0399 - mae: 0.1415 - val_loss: 3.1854 - val_mae: 0.4849\n\nEpoch 00298: val_mae did not improve from 0.46972\nEpoch 299/300\n115/115 [==============================] - 14s 125ms/step - loss: 0.0404 - mae: 0.1410 - val_loss: 3.1392 - val_mae: 0.4820\n\nEpoch 00299: val_mae did not improve from 0.46972\nEpoch 300/300\n115/115 [==============================] - 15s 127ms/step - loss: 0.0397 - mae: 0.1416 - val_loss: 3.1767 - val_mae: 0.4832\n\nEpoch 00300: ReduceLROnPlateau reducing learning rate to 5.15377605552203e-06.\n\nEpoch 00300: val_mae did not improve from 0.46972\n{'loss': [19.80385398864746, 6.8422136306762695, 4.240483283996582, 3.1754863262176514, 2.5418946743011475, 2.5752017498016357, 2.056842088699341, 1.6903916597366333, 1.4884352684020996, 1.204689860343933, 0.8753329515457153, 0.719168484210968, 0.5826432108879089, 0.5714432001113892, 0.8865999579429626, 1.6284329891204834, 2.014981985092163, 2.0329673290252686, 0.9504279494285583, 0.9291341304779053, 0.754385232925415, 0.48413363099098206, 0.49591174721717834, 0.3914776146411896, 0.36840513348579407, 0.7695838809013367, 0.5680168867111206, 0.8278962969779968, 0.5784989595413208, 0.42915502190589905, 0.27519217133522034, 0.24717094004154205, 0.23296913504600525, 0.19537825882434845, 0.3444124162197113, 0.2997652292251587, 0.25359559059143066, 0.24264609813690186, 0.20045533776283264, 0.18058910965919495, 0.15681536495685577, 0.1711786687374115, 0.17244060337543488, 0.22743675112724304, 0.295270174741745, 0.4246976971626282, 0.6866022944450378, 0.4704244136810303, 0.420291930437088, 0.3356305956840515, 0.28611400723457336, 0.2607046663761139, 0.2750047445297241, 0.25708135962486267, 0.26702171564102173, 0.14929550886154175, 0.14503398537635803, 0.3234322965145111, 0.3956083357334137, 0.24458900094032288, 0.14633089303970337, 0.2614538371562958, 0.1806797832250595, 0.14236263930797577, 0.12439056485891342, 0.11864013224840164, 0.12395983934402466, 0.197677880525589, 0.11734671145677567, 0.16721230745315552, 0.1229974776506424, 0.10187684744596481, 0.10083048790693283, 0.10353253781795502, 0.12226974964141846, 0.08747462183237076, 0.07999414950609207, 0.07918252050876617, 0.08307132869958878, 0.08120489120483398, 0.0910913422703743, 0.1108958050608635, 0.10387355834245682, 0.11136727780103683, 0.08097854256629944, 0.07960289716720581, 0.06844276189804077, 0.0782308280467987, 0.1388002336025238, 0.10156100988388062, 0.08860596269369125, 0.08332571387290955, 0.06714646518230438, 0.07252488285303116, 0.07173288613557816, 0.07133276015520096, 0.12058927863836288, 0.09140566736459732, 0.08664827793836594, 0.1370333731174469, 0.0858410969376564, 0.09324091672897339, 0.07473519444465637, 0.08079969882965088, 0.07143102586269379, 0.06557761877775192, 0.061368077993392944, 0.06435074657201767, 0.06383991241455078, 0.07534004747867584, 0.08068793267011642, 0.06107618659734726, 0.06401704996824265, 0.056660283356904984, 0.055138759315013885, 0.061426516622304916, 0.061897993087768555, 0.05785151571035385, 0.058706291019916534, 0.05526415631175041, 0.05367714911699295, 0.05828317627310753, 0.05941846966743469, 0.05477224290370941, 0.05990374833345413, 0.06371097266674042, 0.055056262761354446, 0.05734926089644432, 0.05746057629585266, 0.05698758736252785, 0.05431653931736946, 0.05516058951616287, 0.05330479517579079, 0.06080394610762596, 0.05536993220448494, 0.05153226852416992, 0.05229496583342552, 0.048865001648664474, 0.056122712790966034, 0.06180652603507042, 0.05498751625418663, 0.05097787827253342, 0.05012597516179085, 0.051062170416116714, 0.0498223751783371, 0.05164720118045807, 0.05306510627269745, 0.052973173558712006, 0.0507979691028595, 0.05080024525523186, 0.050771795213222504, 0.04889092966914177, 0.04700728505849838, 0.049636662006378174, 0.050858233124017715, 0.04667644947767258, 0.04781434312462807, 0.0487375482916832, 0.04762531816959381, 0.04729088768362999, 0.04547673463821411, 0.047007761895656586, 0.0466085784137249, 0.06576177477836609, 0.047401733696460724, 0.05120088905096054, 0.047104623168706894, 0.04801646247506142, 0.047900084406137466, 0.048086583614349365, 0.04486294835805893, 0.04520047456026077, 0.04822566732764244, 0.04515742510557175, 0.04342861473560333, 0.04853442311286926, 0.044997911900281906, 0.04576189070940018, 0.04499340429902077, 0.04787040501832962, 0.04470708593726158, 0.043826308101415634, 0.04495133459568024, 0.04724719747900963, 0.0461568608880043, 0.04628222808241844, 0.04374461993575096, 0.042774882167577744, 0.04414360970258713, 0.04380299150943756, 0.0437786690890789, 0.05339458957314491, 0.04892902448773384, 0.04762394726276398, 0.042786549776792526, 0.04409293457865715, 0.04310709238052368, 0.0437324121594429, 0.04311162605881691, 0.04510633274912834, 0.0433133989572525, 0.04208892583847046, 0.04223240166902542, 0.041958216577768326, 0.041866544634103775, 0.050683289766311646, 0.04331350326538086, 0.04385780543088913, 0.04429894685745239, 0.04330193251371384, 0.041487567126750946, 0.04247605428099632, 0.042748596519231796, 0.042179279029369354, 0.0429844856262207, 0.04223543405532837, 0.0435979962348938, 0.0468791127204895, 0.0426039956510067, 0.04081537202000618, 0.04235091060400009, 0.04345906153321266, 0.04114620015025139, 0.04258038476109505, 0.04054723680019379, 0.041024722158908844, 0.04226595535874367, 0.03977023810148239, 0.04320701211690903, 0.041717059910297394, 0.040873780846595764, 0.039378803223371506, 0.041581813246011734, 0.04184994101524353, 0.04179500415921211, 0.04042363911867142, 0.04056469723582268, 0.0459340363740921, 0.040009237825870514, 0.04017895832657814, 0.04170892760157585, 0.04334929585456848, 0.04023202508687973, 0.04205525293946266, 0.04190720245242119, 0.04077521711587906, 0.041849374771118164, 0.04253699630498886, 0.03971060737967491, 0.040016695857048035, 0.040178440511226654, 0.040084924548864365, 0.04256366193294525, 0.03955645114183426, 0.04338166490197182, 0.041854213923215866, 0.0400930754840374, 0.04257621616125107, 0.039857327938079834, 0.04071291163563728, 0.039663564413785934, 0.04024849086999893, 0.04046918451786041, 0.039656784385442734, 0.03995193913578987, 0.04010067507624626, 0.0391397550702095, 0.03828828036785126, 0.0402258075773716, 0.0406835563480854, 0.043170347809791565, 0.041912831366062164, 0.04200035706162453, 0.0412113256752491, 0.04089193418622017, 0.039943303912878036, 0.039498165249824524, 0.040503568947315216, 0.04165439307689667, 0.04000978544354439, 0.04031405225396156, 0.03856495022773743, 0.039655618369579315, 0.0416254885494709, 0.040765129029750824, 0.038671866059303284, 0.03977476805448532, 0.038791775703430176, 0.040362369269132614, 0.056481752544641495, 0.04049336165189743, 0.03951258957386017, 0.03925623744726181, 0.038446418941020966, 0.0401826873421669, 0.03905637189745903, 0.04085181653499603, 0.03987063094973564, 0.04035601392388344, 0.03970704227685928], 'mae': [3.2549021244049072, 1.901334285736084, 1.4762457609176636, 1.262302041053772, 1.1249667406082153, 1.1196218729019165, 0.9805102944374084, 0.9182947874069214, 0.849936306476593, 0.7603490948677063, 0.6533888578414917, 0.5982937216758728, 0.5340432524681091, 0.5220345854759216, 0.589741051197052, 0.8055206537246704, 0.8530108332633972, 0.8882847428321838, 0.6311101913452148, 0.6010867953300476, 0.5428741574287415, 0.4644632339477539, 0.459190309047699, 0.4187852144241333, 0.4121253788471222, 0.5162692666053772, 0.47389888763427734, 0.5443390011787415, 0.4801258146762848, 0.42438405752182007, 0.3582971394062042, 0.33532488346099854, 0.3335854113101959, 0.3108426034450531, 0.35307782888412476, 0.3476654887199402, 0.3368535041809082, 0.3252919912338257, 0.3109433352947235, 0.2973569333553314, 0.2787315547466278, 0.2873830199241638, 0.2841958999633789, 0.3013448417186737, 0.3309335708618164, 0.39936551451683044, 0.4475344121456146, 0.4071938693523407, 0.38670581579208374, 0.3601101338863373, 0.3251143991947174, 0.3180159330368042, 0.318132221698761, 0.29644688963890076, 0.3103835880756378, 0.26883140206336975, 0.2615589201450348, 0.3088536560535431, 0.3659583330154419, 0.2987891137599945, 0.2570551633834839, 0.3031214475631714, 0.27715766429901123, 0.24866536259651184, 0.2393409162759781, 0.2351619154214859, 0.23255281150341034, 0.2616144120693207, 0.23347234725952148, 0.24574458599090576, 0.23503677546977997, 0.22271376848220825, 0.21666070818901062, 0.21853899955749512, 0.2258210927248001, 0.21140938997268677, 0.2032306045293808, 0.2009047567844391, 0.20508769154548645, 0.20042827725410461, 0.20899581909179688, 0.21887165307998657, 0.21188560128211975, 0.22566069662570953, 0.20121543109416962, 0.19839848577976227, 0.18964625895023346, 0.1912449598312378, 0.2339811474084854, 0.2114434689283371, 0.20778758823871613, 0.20584121346473694, 0.18681353330612183, 0.192947119474411, 0.18989278376102448, 0.18408511579036713, 0.2160673290491104, 0.205970898270607, 0.19718188047409058, 0.21871492266654968, 0.20317602157592773, 0.20117585361003876, 0.19063018262386322, 0.19159960746765137, 0.1872166097164154, 0.18090444803237915, 0.17800970375537872, 0.17990808188915253, 0.18124926090240479, 0.18707112967967987, 0.19108758866786957, 0.17745772004127502, 0.181363046169281, 0.17259453237056732, 0.17047365009784698, 0.1756572425365448, 0.17546629905700684, 0.1715262532234192, 0.17345470190048218, 0.16947542130947113, 0.16797977685928345, 0.17155136168003082, 0.1721997708082199, 0.16869176924228668, 0.16973759233951569, 0.17225603759288788, 0.16820484399795532, 0.1694793552160263, 0.17059601843357086, 0.16639772057533264, 0.16773925721645355, 0.16913031041622162, 0.16546957194805145, 0.17211952805519104, 0.16439327597618103, 0.16420148313045502, 0.1634937971830368, 0.16055671870708466, 0.16586697101593018, 0.17556609213352203, 0.1651821881532669, 0.1598007082939148, 0.16131575405597687, 0.1601267009973526, 0.1583154946565628, 0.1617322862148285, 0.16415289044380188, 0.16458351910114288, 0.1589859575033188, 0.1611105501651764, 0.16042295098304749, 0.15933899581432343, 0.15707726776599884, 0.1582944691181183, 0.15742018818855286, 0.1563294529914856, 0.15653318166732788, 0.15793681144714355, 0.15511798858642578, 0.15513718128204346, 0.15115374326705933, 0.15633830428123474, 0.15445558726787567, 0.16056805849075317, 0.15553896129131317, 0.15761175751686096, 0.1527300924062729, 0.15660282969474792, 0.15488874912261963, 0.15668882429599762, 0.15074804425239563, 0.15210731327533722, 0.15300627052783966, 0.15260259807109833, 0.15060625970363617, 0.15356364846229553, 0.15298664569854736, 0.15297658741474152, 0.15212413668632507, 0.15239527821540833, 0.15120919048786163, 0.14859139919281006, 0.15072356164455414, 0.14969243109226227, 0.1528308093547821, 0.15396840870380402, 0.1503046303987503, 0.14812903106212616, 0.14795298874378204, 0.14979754388332367, 0.14916324615478516, 0.14810656011104584, 0.1619119644165039, 0.15365654230117798, 0.147867813706398, 0.14742442965507507, 0.14731121063232422, 0.14486537873744965, 0.14847004413604736, 0.15138855576515198, 0.1479572355747223, 0.14713069796562195, 0.14598536491394043, 0.1466933935880661, 0.14328885078430176, 0.15070290863513947, 0.14612232148647308, 0.1465384066104889, 0.1494009792804718, 0.1470448225736618, 0.14390335977077484, 0.1459570974111557, 0.14823272824287415, 0.1452217698097229, 0.14525383710861206, 0.14504384994506836, 0.1481088101863861, 0.1491517722606659, 0.14580616354942322, 0.1432197540998459, 0.14624544978141785, 0.14737702906131744, 0.1435939222574234, 0.14705419540405273, 0.14438511431217194, 0.1449349820613861, 0.1448037475347519, 0.142677903175354, 0.14520905911922455, 0.14552360773086548, 0.14558467268943787, 0.14340952038764954, 0.14463281631469727, 0.14519155025482178, 0.14364179968833923, 0.1427323967218399, 0.1427633911371231, 0.14598298072814941, 0.14262433350086212, 0.14306998252868652, 0.14597231149673462, 0.14475418627262115, 0.14304055273532867, 0.14514347910881042, 0.14450089633464813, 0.14348047971725464, 0.14226704835891724, 0.14581134915351868, 0.1418730765581131, 0.14285245537757874, 0.14270736277103424, 0.1429751068353653, 0.14249753952026367, 0.14184312522411346, 0.14440028369426727, 0.14501553773880005, 0.14312157034873962, 0.14291070401668549, 0.1413685828447342, 0.14425691962242126, 0.14232122898101807, 0.14290274679660797, 0.1426163762807846, 0.14109331369400024, 0.14243444800376892, 0.14180532097816467, 0.1416534036397934, 0.13854995369911194, 0.1426551342010498, 0.14336811006069183, 0.1444522887468338, 0.14366498589515686, 0.1436367928981781, 0.14216859638690948, 0.14324554800987244, 0.14230109751224518, 0.1396058350801468, 0.14161425828933716, 0.14457985758781433, 0.14034853875637054, 0.1420164704322815, 0.1396910846233368, 0.14014747738838196, 0.14280769228935242, 0.14263123273849487, 0.1388881355524063, 0.14076413214206696, 0.14066123962402344, 0.14314588904380798, 0.14341726899147034, 0.14180122315883636, 0.14130166172981262, 0.1404247283935547, 0.13854506611824036, 0.14367787539958954, 0.14092259109020233, 0.14286084473133087, 0.14150868356227875, 0.14104993641376495, 0.1416066735982895], 'val_loss': [31.797420501708984, 24.539207458496094, 25.6820125579834, 28.805644989013672, 22.712879180908203, 55.93759536743164, 29.09864616394043, 69.45173645019531, 60.24937438964844, 28.107521057128906, 28.97146224975586, 31.041597366333008, 25.888338088989258, 26.97244644165039, 26.032909393310547, 37.836204528808594, 34.922691345214844, 24.269258499145508, 25.33154296875, 18.707561492919922, 20.280414581298828, 28.640731811523438, 26.150484085083008, 10.274903297424316, 53.18928146362305, 46.18082046508789, 25.17705726623535, 28.7365665435791, 25.7711238861084, 28.796850204467773, 22.379636764526367, 14.536480903625488, 26.622140884399414, 5.260125160217285, 26.233009338378906, 25.01729393005371, 24.942203521728516, 25.311628341674805, 11.468042373657227, 20.662778854370117, 21.350727081298828, 26.025230407714844, 26.16620635986328, 25.715818405151367, 1080629.625, 2618.353271484375, 39.69523239135742, 67.16678619384766, 26.65940284729004, 24.67715835571289, 26.336322784423828, 25.874401092529297, 26.598812103271484, 24.433366775512695, 25.207311630249023, 24.36713981628418, 6.935392379760742, 25.728681564331055, 21.95240020751953, 8.744241714477539, 25.068700790405273, 53.403404235839844, 48.05192947387695, 20.354427337646484, 4.872174263000488, 48.07923889160156, 26.62308692932129, 18.56431770324707, 18.12761688232422, 23.353797912597656, 39.45088195800781, 15.930228233337402, 17.59598731994629, 30.749664306640625, 19.29145622253418, 6.590761184692383, 10.241159439086914, 14.848387718200684, 4.855984210968018, 18.133604049682617, 26.33497428894043, 38.97054672241211, 37.95054626464844, 15.149367332458496, 20.19846534729004, 22.40690803527832, 22.646995544433594, 21.82656478881836, 20.79389762878418, 20.568788528442383, 26.528118133544922, 26.009641647338867, 12.43450927734375, 6.026288986206055, 16.173208236694336, 26.26517105102539, 25.224201202392578, 10.848319053649902, 18.77661895751953, 50.03145980834961, 25.6404972076416, 26.663284301757812, 8.567502975463867, 13.21770191192627, 12.053238868713379, 21.866130828857422, 12.900808334350586, 6.371107578277588, 9.381240844726562, 29.35364532470703, 10.42504596710205, 3.662888288497925, 20.448284149169922, 3.9461398124694824, 9.144015312194824, 10.224149703979492, 15.539074897766113, 9.033093452453613, 8.01676082611084, 3.363255500793457, 3.7950687408447266, 7.2092084884643555, 18.268972396850586, 3.6965513229370117, 17.31405258178711, 14.123373985290527, 4.152288913726807, 4.6847310066223145, 5.736207962036133, 14.86462688446045, 12.303510665893555, 5.162092685699463, 4.528564453125, 5.967771530151367, 18.636289596557617, 11.290634155273438, 3.2791390419006348, 5.203428745269775, 8.705493927001953, 20.065874099731445, 7.448398113250732, 13.76125717163086, 4.731513977050781, 3.3248322010040283, 4.253654479980469, 4.6491875648498535, 3.90313720703125, 10.628313064575195, 3.926239252090454, 7.073511600494385, 8.763489723205566, 7.510860919952393, 7.147030830383301, 5.061696529388428, 18.87379264831543, 7.2658233642578125, 3.6178317070007324, 3.285423517227173, 3.276841163635254, 4.92842435836792, 10.354447364807129, 4.538002967834473, 4.181405544281006, 16.152633666992188, 3.4736101627349854, 12.189253807067871, 11.234222412109375, 16.22089385986328, 3.6597530841827393, 3.79982328414917, 3.092682123184204, 2.893484354019165, 3.8709964752197266, 3.0140273571014404, 3.6701087951660156, 10.394859313964844, 6.6839919090271, 3.7364139556884766, 6.50862455368042, 3.669574499130249, 6.461885452270508, 3.3161725997924805, 4.06492280960083, 3.4337620735168457, 4.978877067565918, 3.4133501052856445, 3.3869073390960693, 3.4294979572296143, 22.408926010131836, 7.7801032066345215, 3.221181631088257, 4.580859184265137, 17.99045181274414, 3.725980043411255, 3.1205599308013916, 3.1245059967041016, 3.2034032344818115, 3.435239553451538, 3.1012136936187744, 3.162203073501587, 3.245155096054077, 3.114271402359009, 3.1408586502075195, 3.2170403003692627, 3.1451539993286133, 6.674132347106934, 3.862783432006836, 3.5344791412353516, 4.415624618530273, 3.139740228652954, 3.2578930854797363, 3.4779441356658936, 3.3182454109191895, 3.3226237297058105, 3.215757369995117, 3.7101399898529053, 3.155045509338379, 4.323371410369873, 3.7771568298339844, 3.0668609142303467, 3.031222105026245, 3.416161060333252, 3.022730588912964, 3.030405044555664, 3.0186073780059814, 3.495059013366699, 3.5049357414245605, 3.0401012897491455, 3.0528934001922607, 3.0973849296569824, 3.1192383766174316, 3.108644485473633, 3.047855854034424, 3.1675944328308105, 9.296759605407715, 4.040561676025391, 3.0122108459472656, 6.278398513793945, 3.2558305263519287, 3.148766040802002, 3.163858652114868, 3.115107774734497, 3.082427978515625, 3.00063419342041, 3.056702136993408, 3.001981496810913, 3.638288736343384, 3.1883225440979004, 3.1146445274353027, 3.2063710689544678, 3.2328147888183594, 3.2525758743286133, 3.1217856407165527, 3.3187625408172607, 3.8954551219940186, 3.0299689769744873, 3.0371670722961426, 3.110095977783203, 2.9871599674224854, 3.0011179447174072, 3.0734236240386963, 3.118460178375244, 3.076744794845581, 3.0836524963378906, 3.079174041748047, 3.131826877593994, 3.1395342350006104, 3.1578445434570312, 3.0990726947784424, 3.1499099731445312, 3.3209517002105713, 3.28144907951355, 3.2672269344329834, 3.1857941150665283, 3.1976919174194336, 3.1481211185455322, 3.1826817989349365, 3.1876378059387207, 3.1952807903289795, 3.1223926544189453, 3.0291738510131836, 3.102461099624634, 3.1424739360809326, 3.213329792022705, 3.1716222763061523, 3.0551047325134277, 3.133375644683838, 3.139265298843384, 3.1490113735198975, 3.3541476726531982, 3.1716561317443848, 3.1885628700256348, 3.174152374267578, 3.187896728515625, 3.181744337081909, 3.191941022872925, 3.1923282146453857, 3.185370445251465, 3.1392033100128174, 3.176699638366699], 'val_mae': [4.846127986907959, 4.103734970092773, 3.773223638534546, 3.9967191219329834, 3.88525128364563, 6.013112545013428, 4.07638692855835, 6.885282039642334, 6.2985005378723145, 3.9658913612365723, 4.071128845214844, 4.132082939147949, 3.7469561100006104, 3.8954973220825195, 4.1675310134887695, 4.753615379333496, 4.546684265136719, 3.761497974395752, 3.7714691162109375, 3.0328030586242676, 3.382622480392456, 3.9526946544647217, 3.7852084636688232, 2.113585948944092, 5.810371398925781, 5.197231769561768, 3.7630774974823, 3.9335362911224365, 3.7413086891174316, 4.195323467254639, 3.430328607559204, 2.6133859157562256, 3.7563064098358154, 1.2605130672454834, 3.736091136932373, 3.723668098449707, 3.787766933441162, 3.6955456733703613, 2.275702476501465, 3.20731782913208, 3.3033108711242676, 3.7242069244384766, 3.718172788619995, 3.793519973754883, 974.7696533203125, 42.40812683105469, 4.871372222900391, 6.650534629821777, 3.7696831226348877, 3.918972969055176, 3.776200294494629, 3.7471282482147217, 3.7974185943603516, 3.9789209365844727, 3.9354476928710938, 3.8751344680786133, 1.3827251195907593, 4.257490158081055, 3.480102777481079, 1.722788691520691, 3.760242223739624, 6.273439884185791, 5.991522312164307, 3.221229314804077, 0.958655059337616, 5.791627883911133, 3.756359815597534, 3.0441348552703857, 2.9892656803131104, 3.5691730976104736, 5.436103820800781, 2.6702287197113037, 2.8557043075561523, 4.7223801612854, 3.1178882122039795, 1.4039698839187622, 1.937610387802124, 2.4502692222595215, 0.9962093830108643, 2.8137009143829346, 3.7412147521972656, 5.3549675941467285, 5.309363842010498, 2.520801305770874, 3.1549556255340576, 3.4339892864227295, 3.4134867191314697, 3.286268711090088, 3.2561795711517334, 3.2530364990234375, 3.7449419498443604, 3.7096362113952637, 2.0099549293518066, 1.1616731882095337, 2.7596349716186523, 3.783130407333374, 3.908139944076538, 1.9545180797576904, 3.128727674484253, 5.490569591522217, 3.7779042720794678, 3.758492946624756, 1.7258224487304688, 2.2625033855438232, 2.0835304260253906, 3.3222482204437256, 2.1525468826293945, 1.2540916204452515, 1.674914836883545, 4.0637922286987305, 1.756719708442688, 0.6940550208091736, 3.128481388092041, 0.7176530957221985, 1.515380620956421, 1.592051386833191, 2.3500468730926514, 1.5481419563293457, 1.430798053741455, 0.5995615124702454, 0.6787360906600952, 1.386124849319458, 2.9944989681243896, 0.7059416174888611, 2.7918601036071777, 2.209895372390747, 0.7434861660003662, 0.7766109108924866, 1.0609973669052124, 2.53555965423584, 2.0901260375976562, 0.9749882817268372, 0.8504785299301147, 1.1983131170272827, 2.9421117305755615, 1.9772697687149048, 0.5850520133972168, 0.8886756896972656, 1.467835545539856, 3.1019465923309326, 1.342928409576416, 2.379074811935425, 0.8964178562164307, 0.5743333697319031, 0.7530068755149841, 0.8743177056312561, 0.6698775887489319, 1.721878170967102, 0.7172815799713135, 1.2107142210006714, 1.3904354572296143, 1.3022589683532715, 1.2196823358535767, 0.8998503088951111, 2.809108257293701, 1.2788937091827393, 0.6276562809944153, 0.6326531171798706, 0.5486721992492676, 0.8584361672401428, 1.6515835523605347, 0.7728523015975952, 0.7013260126113892, 2.650332450866699, 0.6208363175392151, 2.02742600440979, 1.9674551486968994, 2.6020305156707764, 0.665248692035675, 0.6784632802009583, 0.5401323437690735, 0.48736080527305603, 0.6648646593093872, 0.5078599452972412, 0.6418281197547913, 1.7638189792633057, 1.2372198104858398, 0.6607473492622375, 1.1128946542739868, 0.6267873644828796, 1.117465853691101, 0.5283190608024597, 0.7045091390609741, 0.5897538065910339, 0.9731824994087219, 0.5506367683410645, 0.5349353551864624, 0.54116290807724, 3.353447437286377, 1.3707460165023804, 0.5010769367218018, 0.7872359156608582, 2.665003538131714, 0.6068689823150635, 0.507602870464325, 0.4987494945526123, 0.49411773681640625, 0.5818548798561096, 0.5142329335212708, 0.5031874775886536, 0.5273507237434387, 0.48267340660095215, 0.49509045481681824, 0.5178524851799011, 0.5012866854667664, 1.1632421016693115, 0.621186375617981, 0.563648521900177, 0.721941351890564, 0.5143362879753113, 0.5241966843605042, 0.5795862674713135, 0.5435305833816528, 0.5414885878562927, 0.5164573788642883, 0.6513973474502563, 0.5173269510269165, 0.7186048626899719, 0.6118946671485901, 0.4768691956996918, 0.47762539982795715, 0.5478251576423645, 0.49203288555145264, 0.4726325571537018, 0.4770432114601135, 0.5747743248939514, 0.5815147161483765, 0.48444312810897827, 0.4966207444667816, 0.4979506731033325, 0.4937624931335449, 0.48601263761520386, 0.4765753149986267, 0.5115032196044922, 1.5260478258132935, 0.6559158563613892, 0.48026859760284424, 1.1113996505737305, 0.533098578453064, 0.5014628767967224, 0.5187672972679138, 0.5185165405273438, 0.5035406351089478, 0.4762059450149536, 0.4697206914424896, 0.476339191198349, 0.5893083214759827, 0.4964751601219177, 0.4845699667930603, 0.5017539858818054, 0.5126253366470337, 0.5275958180427551, 0.4930207133293152, 0.5685526728630066, 0.6553382873535156, 0.4731994569301605, 0.4700247347354889, 0.48108136653900146, 0.48021936416625977, 0.4702233076095581, 0.4753228724002838, 0.4771609306335449, 0.4752379357814789, 0.47381871938705444, 0.47945499420166016, 0.48374539613723755, 0.49053680896759033, 0.49548059701919556, 0.4857315421104431, 0.4961278736591339, 0.5357818603515625, 0.5362093448638916, 0.5100914239883423, 0.4941166341304779, 0.4868311285972595, 0.47904473543167114, 0.48650237917900085, 0.5002397894859314, 0.49709099531173706, 0.4784454107284546, 0.5067061185836792, 0.47350654006004333, 0.4826575517654419, 0.496941477060318, 0.48392003774642944, 0.474115788936615, 0.48186588287353516, 0.4794023334980011, 0.47979646921157837, 0.5518386960029602, 0.49669426679611206, 0.4931861162185669, 0.4836422801017761, 0.4895741045475006, 0.4886525571346283, 0.4925478994846344, 0.48550671339035034, 0.48491352796554565, 0.4820239245891571, 0.48319220542907715], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.00090000004, 0.00090000004, 0.00090000004, 0.00090000004, 0.00090000004, 0.00090000004, 0.00090000004, 0.00090000004, 0.00090000004, 0.00090000004, 0.00081000006, 0.00081000006, 0.00081000006, 0.00081000006, 0.00081000006, 0.00081000006, 0.00081000006, 0.00081000006, 0.00081000006, 0.00081000006, 0.00081000006, 0.0007290001, 0.0007290001, 0.0007290001, 0.0007290001, 0.0007290001, 0.0007290001, 0.0007290001, 0.0007290001, 0.0007290001, 0.0007290001, 0.00065610005, 0.00065610005, 0.00065610005, 0.00065610005, 0.00065610005, 0.00059049006, 0.00059049006, 0.00059049006, 0.00059049006, 0.00059049006, 0.00053144107, 0.00053144107, 0.00053144107, 0.00053144107, 0.00053144107, 0.00047829695, 0.00047829695, 0.00047829695, 0.00047829695, 0.00047829695, 0.00043046725, 0.00043046725, 0.00043046725, 0.00043046725, 0.00043046725, 0.00038742053, 0.00038742053, 0.00038742053, 0.00038742053, 0.00038742053, 0.00038742053, 0.00034867847, 0.00034867847, 0.00034867847, 0.00034867847, 0.00034867847, 0.00031381063, 0.00031381063, 0.00031381063, 0.00031381063, 0.00031381063, 0.00028242957, 0.00028242957, 0.00028242957, 0.00028242957, 0.00028242957, 0.0002541866, 0.0002541866, 0.0002541866, 0.0002541866, 0.0002541866, 0.00022876794, 0.00022876794, 0.00022876794, 0.00022876794, 0.00022876794, 0.00020589115, 0.00020589115, 0.00020589115, 0.00020589115, 0.00020589115, 0.00018530204, 0.00018530204, 0.00018530204, 0.00018530204, 0.00018530204, 0.00016677183, 0.00016677183, 0.00016677183, 0.00016677183, 0.00016677183, 0.00015009465, 0.00015009465, 0.00015009465, 0.00015009465, 0.00015009465, 0.00015009465, 0.00015009465, 0.00013508518, 0.00013508518, 0.00013508518, 0.00013508518, 0.00013508518, 0.00013508518, 0.00013508518, 0.00013508518, 0.00012157666, 0.00012157666, 0.00012157666, 0.00012157666, 0.00012157666, 0.000109418994, 0.000109418994, 0.000109418994, 0.000109418994, 0.000109418994, 9.8477096e-05, 9.8477096e-05, 9.8477096e-05, 9.8477096e-05, 9.8477096e-05, 9.8477096e-05, 9.8477096e-05, 8.8629386e-05, 8.8629386e-05, 8.8629386e-05, 8.8629386e-05, 8.8629386e-05, 8.8629386e-05, 8.8629386e-05, 7.976645e-05, 7.976645e-05, 7.976645e-05, 7.976645e-05, 7.976645e-05, 7.17898e-05, 7.17898e-05, 7.17898e-05, 7.17898e-05, 7.17898e-05, 7.17898e-05, 7.17898e-05, 7.17898e-05, 7.17898e-05, 7.17898e-05, 6.4610824e-05, 6.4610824e-05, 6.4610824e-05, 6.4610824e-05, 6.4610824e-05, 5.814974e-05, 5.814974e-05, 5.814974e-05, 5.814974e-05, 5.814974e-05, 5.814974e-05, 5.814974e-05, 5.814974e-05, 5.2334766e-05, 5.2334766e-05, 5.2334766e-05, 5.2334766e-05, 5.2334766e-05, 4.710129e-05, 4.710129e-05, 4.710129e-05, 4.710129e-05, 4.710129e-05, 4.2391162e-05, 4.2391162e-05, 4.2391162e-05, 4.2391162e-05, 4.2391162e-05, 3.8152048e-05, 3.8152048e-05, 3.8152048e-05, 3.8152048e-05, 3.8152048e-05, 3.4336845e-05, 3.4336845e-05, 3.4336845e-05, 3.4336845e-05, 3.4336845e-05, 3.4336845e-05, 3.4336845e-05, 3.4336845e-05, 3.4336845e-05, 3.4336845e-05, 3.090316e-05, 3.090316e-05, 3.090316e-05, 3.090316e-05, 3.090316e-05, 2.7812845e-05, 2.7812845e-05, 2.7812845e-05, 2.7812845e-05, 2.7812845e-05, 2.503156e-05, 2.503156e-05, 2.503156e-05, 2.503156e-05, 2.503156e-05, 2.503156e-05, 2.503156e-05, 2.503156e-05, 2.503156e-05, 2.503156e-05, 2.503156e-05, 2.503156e-05, 2.2528404e-05, 2.2528404e-05, 2.2528404e-05, 2.2528404e-05, 2.2528404e-05, 2.0275564e-05, 2.0275564e-05, 2.0275564e-05, 2.0275564e-05, 2.0275564e-05, 1.8248007e-05, 1.8248007e-05, 1.8248007e-05, 1.8248007e-05, 1.8248007e-05, 1.6423206e-05, 1.6423206e-05, 1.6423206e-05, 1.6423206e-05, 1.6423206e-05, 1.6423206e-05, 1.4780885e-05, 1.4780885e-05, 1.4780885e-05, 1.4780885e-05, 1.4780885e-05, 1.3302797e-05, 1.3302797e-05, 1.3302797e-05, 1.3302797e-05, 1.3302797e-05, 1.1972517e-05, 1.1972517e-05, 1.1972517e-05, 1.1972517e-05, 1.1972517e-05, 1.0775266e-05, 1.0775266e-05, 1.0775266e-05, 1.0775266e-05, 1.0775266e-05, 9.697739e-06, 9.697739e-06, 9.697739e-06, 9.697739e-06, 9.697739e-06, 8.727965e-06, 8.727965e-06, 8.727965e-06, 8.727965e-06, 8.727965e-06, 7.855168e-06, 7.855168e-06, 7.855168e-06, 7.855168e-06, 7.855168e-06, 7.0696515e-06, 7.0696515e-06, 7.0696515e-06, 7.0696515e-06, 7.0696515e-06, 6.3626862e-06, 6.3626862e-06, 6.3626862e-06, 6.3626862e-06, 6.3626862e-06, 5.726418e-06, 5.726418e-06, 5.726418e-06, 5.726418e-06, 5.726418e-06]}\n--- 4394.156490802765 seconds ---\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights(checkpoint_filepath)","metadata":{"execution":{"iopub.status.busy":"2022-12-23T14:40:01.276510Z","iopub.execute_input":"2022-12-23T14:40:01.276857Z","iopub.status.idle":"2022-12-23T14:40:02.691870Z","shell.execute_reply.started":"2022-12-23T14:40:01.276828Z","shell.execute_reply":"2022-12-23T14:40:02.690762Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f55222c5050>"},"metadata":{}}]},{"cell_type":"code","source":"converter = tf.lite.TFLiteConverter.from_keras_model(model)\ntensorflow_lite_model = converter.convert()\n\nwith open('best_model.tflite', 'wb') as f:\n    f.write(tensorflow_lite_model)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-23T14:40:02.699265Z","iopub.execute_input":"2022-12-23T14:40:02.699803Z","iopub.status.idle":"2022-12-23T14:40:42.744384Z","shell.execute_reply.started":"2022-12-23T14:40:02.699773Z","shell.execute_reply":"2022-12-23T14:40:42.743283Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"2022-12-23 14:40:15.216852: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n/opt/conda/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n  category=CustomMaskWarning)\n2022-12-23 14:40:40.637088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 14:40:40.637728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 14:40:40.638431: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\n2022-12-23 14:40:40.638542: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n2022-12-23 14:40:40.639014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 14:40:40.639456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 14:40:40.640344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 14:40:40.640799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 14:40:40.641502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 14:40:40.641933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 14:40:40.642808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 14:40:40.643229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 14:40:40.643957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 14:40:40.644302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13789 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n2022-12-23 14:40:40.644407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-23 14:40:40.645050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13789 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n2022-12-23 14:40:40.672837: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n  function_optimizer: function_optimizer did nothing. time = 0.008ms.\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n\n2022-12-23 14:40:42.130121: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n2022-12-23 14:40:42.130172: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n2022-12-23 14:40:42.326885: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(x_test, batch_size=64)\npreds[:15]","metadata":{"execution":{"iopub.status.busy":"2022-12-23T14:40:42.747205Z","iopub.execute_input":"2022-12-23T14:40:42.750019Z","iopub.status.idle":"2022-12-23T14:40:44.857596Z","shell.execute_reply.started":"2022-12-23T14:40:42.749987Z","shell.execute_reply":"2022-12-23T14:40:44.856572Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"array([[ 5.9529586],\n       [ 6.955743 ],\n       [ 7.0574546],\n       [18.994774 ],\n       [17.971582 ],\n       [10.931372 ],\n       [ 7.030874 ],\n       [ 3.0231957],\n       [ 6.004582 ],\n       [ 3.0452628],\n       [13.936548 ],\n       [ 7.0187516],\n       [ 6.0264144],\n       [ 6.98759  ],\n       [17.945398 ]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"y_test[:15]","metadata":{"execution":{"iopub.status.busy":"2022-12-23T14:40:44.858965Z","iopub.execute_input":"2022-12-23T14:40:44.859332Z","iopub.status.idle":"2022-12-23T14:40:44.866128Z","shell.execute_reply.started":"2022-12-23T14:40:44.859297Z","shell.execute_reply":"2022-12-23T14:40:44.865154Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"array([ 6,  7, 10, 14, 18, 11,  7,  3,  6,  3, 14,  7,  6,  7, 18])"},"metadata":{}}]},{"cell_type":"code","source":"y_test[:15]","metadata":{"execution":{"iopub.status.busy":"2022-12-23T14:40:44.867892Z","iopub.execute_input":"2022-12-23T14:40:44.868510Z","iopub.status.idle":"2022-12-23T14:40:44.879114Z","shell.execute_reply.started":"2022-12-23T14:40:44.868451Z","shell.execute_reply":"2022-12-23T14:40:44.877780Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"array([ 6,  7, 10, 14, 18, 11,  7,  3,  6,  3, 14,  7,  6,  7, 18])"},"metadata":{}}]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-12-23T14:40:44.880738Z","iopub.execute_input":"2022-12-23T14:40:44.881127Z","iopub.status.idle":"2022-12-23T14:40:45.201665Z","shell.execute_reply.started":"2022-12-23T14:40:44.881051Z","shell.execute_reply":"2022-12-23T14:40:45.200690Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"38208"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2022-12-23T14:40:45.203119Z","iopub.execute_input":"2022-12-23T14:40:45.203488Z","iopub.status.idle":"2022-12-23T14:40:45.210716Z","shell.execute_reply.started":"2022-12-23T14:40:45.203450Z","shell.execute_reply":"2022-12-23T14:40:45.209675Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def get_classification_metric(testy, probs):\n    from sklearn.metrics import precision_recall_curve\n    precision, recall, thresholds = precision_recall_curve(testy, probs)\n    # convert to f score\n    fscore = 2 * (precision * recall) / (precision + recall)\n    # locate the index of the largest f score\n    ix = np.argmax(fscore)\n    return fscore[ix]","metadata":{"execution":{"iopub.status.busy":"2022-12-23T14:40:45.212053Z","iopub.execute_input":"2022-12-23T14:40:45.213025Z","iopub.status.idle":"2022-12-23T14:40:45.223366Z","shell.execute_reply.started":"2022-12-23T14:40:45.212989Z","shell.execute_reply":"2022-12-23T14:40:45.222494Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy: \", get_classification_metric(y_test.round(), preds.round())*100)","metadata":{"execution":{"iopub.status.busy":"2022-12-23T14:40:45.225952Z","iopub.execute_input":"2022-12-23T14:40:45.226838Z","iopub.status.idle":"2022-12-23T14:40:45.534140Z","shell.execute_reply.started":"2022-12-23T14:40:45.226801Z","shell.execute_reply":"2022-12-23T14:40:45.532271Z"},"trusted":true},"execution_count":25,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/3982110075.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_classification_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_23/414416569.py\u001b[0m in \u001b[0;36mget_classification_metric\u001b[0;34m(testy, probs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_classification_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# convert to f score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mprecision_recall_curve\u001b[0;34m(y_true, probas_pred, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    857\u001b[0m     \"\"\"\n\u001b[1;32m    858\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobas_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m     )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: multiclass format is not supported"],"ename":"ValueError","evalue":"multiclass format is not supported","output_type":"error"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nprint(ConfusionMatrixDisplay.from_predictions(y_test.round(), preds.round(), normalize=None))\n\nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)\nplt.show()\nplt.savefig('Confusion_Mat.png')\n","metadata":{"execution":{"iopub.status.busy":"2022-12-23T14:42:41.490292Z","iopub.execute_input":"2022-12-23T14:42:41.490655Z","iopub.status.idle":"2022-12-23T14:42:42.798630Z","shell.execute_reply.started":"2022-12-23T14:42:41.490624Z","shell.execute_reply":"2022-12-23T14:42:42.797769Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x7f5504bcc990>\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 0 Axes>"},"metadata":{}}]},{"cell_type":"code","source":"print(accuracy_score(y_test.round(), preds.round(), normalize=True, sample_weight=None)*100)","metadata":{"execution":{"iopub.status.busy":"2022-12-23T14:42:58.719523Z","iopub.execute_input":"2022-12-23T14:42:58.719907Z","iopub.status.idle":"2022-12-23T14:42:58.727170Z","shell.execute_reply.started":"2022-12-23T14:42:58.719875Z","shell.execute_reply":"2022-12-23T14:42:58.726018Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"89.2248062015504\n","output_type":"stream"}]},{"cell_type":"code","source":"\n#print(history.history)\nplt.plot(history.history['mae'])\nplt.title('Face Detection Mae')\nplt.xlabel('Epoch')\nplt.ylabel('mae')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-23T14:43:36.682968Z","iopub.execute_input":"2022-12-23T14:43:36.683320Z","iopub.status.idle":"2022-12-23T14:43:36.875653Z","shell.execute_reply.started":"2022-12-23T14:43:36.683292Z","shell.execute_reply":"2022-12-23T14:43:36.874635Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAptUlEQVR4nO3deZhcdZ3v8fe3lu6u3judTshGOglhX2OGxWVEFEVGxXHwDugo8qB4cZ/RuS4zj9uM19FRdBAGRGEU9AruooKACCgihAQSIIRANsieTqf3pbqr6nv/OKc71d3VSSekutI5n9fz1EPVOaeqvqcr1Kd+v985v2PujoiIRFes1AWIiEhpKQhERCJOQSAiEnEKAhGRiFMQiIhEnIJARCTiFAQiJWZm3Wa2sNR1SHQpCGRSmNkmM+sLv/SGbrMn6b3dzHrC92w1s/vM7O8P4PnvMbOHDlEtD5jZe/OXuXu1u284FK8/6r02mdmAmU0ftfyJ8G/SfKjfU6YmBYFMpjeHX3pDt22T+N6nuXs1cBzwPeBaM/vcJL5/qWwELh16YGanAJWlK0cORwoCKRkzazCz35hZi5m1hffn5q2fZmb/Y2bbwvW/zFv3JjNbaWbtZvawmZ06kfd0993ufitwFfBpM2sMX6/OzG4ys+1mttXM/t3M4mZ2AnADcE7YomgPty83s6+Z2YtmttPMbjCzVF59F4X1dZrZejO7wMy+BLyKIIS6zezacFs3s2Py6rgl/Ju8YGb/amaxcN17zOyh8H3bzGyjmb1xP7t8K/DuvMeXAbeM+hz+JmwldJrZZjP7/Kj1Z4d/43YzW2Vm507kby1TiLvrplvRb8Am4HWjljUCf0fwC7UG+Anwy7z1vwVuBxqAJPDqcPkZwC7gLCBO8OW2CSgf570dOGbUsiSQAd4YPv4F8G2gCpgBLAPeH657D/DQqOd/A7gDmBbW/mvgy+G6M4EO4HyCH1tzgOPDdQ8A7x2vPoIv6V+Fr9kMPAdckVfHIPC+cL+vArYBtq+/ObAWOCF8zhZgfviezeF25wKnhLWeCuwE3hqumwO0AheG688PHzeV+t+UbofuVvICdIvGLfxS6gbaw9svC2xzOtAW3p8F5ICGAttdD/zbqGVrh4KiwPZjgiBcvgN4JzATSAOpvHWXAveH90cEAWBAD7Aob9k5wMbw/reBb4xTy7hBEH5RDwAn5q17P/BAXh3r8tZVhs89ah9/89cB/wp8GbgAuBdI5AdBged9c6h+4JPAraPW3w1cVup/U7odulsCkcnzVnf//dADM6sk+GV9AcGvfoAaM4sD84A97t5W4HXmA5eZ2YfzlpUBEx58NrMk0ATsCV8vCWw3s6FNYsDmcZ7eRPAlvCJveyP4Iies/c6J1pJneljHC3nLXiD4VT5kx9Add+8N3796P697K/BHYAGjuoUAzOws4D+Akwn+juUErTMI/jZvN7M35z0lCdy//92RqUJBIKX0cYLB27PcfYeZnQ48QfCluhmYZmb17t4+6nmbgS+5+5dewntfRNA1tIzgyy8NTHf3TIFtR0/RuxvoA05y960Ftt8MLBrnffc13e9ugq6f+cAz4bKjgULvMWHu/oKZbSTo3rmiwCb/D7iWoJus38y+SRBKEOzLre7+vpdSgxzeNFgspVRD8IXabmbTgOGjeNx9O3AX8N/hoHLSzP46XP0d4H+b2VkWqAoHPGv294bhAPQ7geuAr7h7a/he9wBfN7NaM4uZ2SIze3X4tJ3AXDMrC2vLhTV8w8xmhK87x8zeEG5/E3C5mb02fK05ZnZ83msVPGfA3bPAj4EvmVmNmc0H/gn4wX7/kvt3BXCeu/cUWFdD0PrqN7MzgXfkrfsB8GYze0M4eF5hZufmD+rL1KcgkFL6JpAi+CX8CPC7UevfRfAL+VmCweGPAbj7coIB02uBNmAdQf/5vqwys+5w2/cC/+jun81b/26ClsEz4Wv+lGCcAuAPwGpgh5ntDpd9MnytR8ysE/g9QesGd18GXE7Q7dUBPEjwKx/gv4CLw6N+rilQ54cJxh82AA8R/Fq/eT/7tl/uvj78uxXyAeCLZtYFfJYgjIaet5mg9fQZoIWghfDP6LvjiGLuujCNiEiUKdVFRCJOQSAiEnEKAhGRiFMQiIhE3JQ7j2D69One3Nxc6jJERKaUFStW7Hb3pkLrplwQNDc3s3z5eEfBiYhIIWb2wnjr1DUkIhJxCgIRkYhTEIiIRJyCQEQk4hQEIiIRpyAQEYk4BYGISMRFJgjW7uji6/esZXd3utSliIgcViITBOt2dfOtP6yjtXug1KWIiBxWIhME8XBPszldf0FEJF9kgiAWXmQ8pwvxiIiMEJkgiMeCIFCLQERkpMgEQWwoCNQiEBEZITJBEB/qGlKLQERkhOgEgbqGREQKikwQDA0Wq2tIRGSkyATBUIsglytxISIih5kIBUHwX7UIRERGikwQxDRYLCJSUGSCQIPFIiKFRSYIhloEGQWBiMgIRQsCM6sws2VmtsrMVpvZFwpsU25mt5vZOjN71Myai1VPIq4pJkRECilmiyANnOfupwGnAxeY2dmjtrkCaHP3Y4BvAF8pVjFDJ5Spa0hEZKSiBYEHusOHyfA2+lv4IuD74f2fAq81C7+xD7GhKSbUIhARGamoYwRmFjezlcAu4F53f3TUJnOAzQDungE6gMYCr3OlmS03s+UtLS0HVYtaBCIihRU1CNw96+6nA3OBM83s5IN8nRvdfam7L21qajqoWnTUkIhIYZNy1JC7twP3AxeMWrUVmAdgZgmgDmgtRg3qGhIRKayYRw01mVl9eD8FnA88O2qzO4DLwvsXA39wL8439d6uoWK8uojI1JUo4mvPAr5vZnGCwPmxu//GzL4ILHf3O4CbgFvNbB2wB7ikWMXENMWEiEhBRQsCd38SOKPA8s/m3e8H3l6sGvLpegQiIoVF5sxiDRaLiBQWmSDQYLGISGGRCQKdRyAiUlh0gkAXrxcRKSgyQaDrEYiIFBaZINg7WFziQkREDjORCYIwB9Q1JCIySmSCwMyImbqGRERGi0wQQNA9pBaBiMhIkQqCmJlaBCIio0QqCOIx03kEIiKjRCsITF1DIiKjRSoIYjF1DYmIjBapINBgsYjIWJEKgpiZTigTERklUkEQj+k8AhGR0aIVBBosFhEZI1JBENPhoyIiY0QqCHQegYjIWNELAnUNiYiMEK0g0BQTIiJjRCsI1DUkIjJGpIIgZqaL14uIjFK0IDCzeWZ2v5k9Y2arzeyjBbY518w6zGxlePtsseoBtQhERApJFPG1M8DH3f1xM6sBVpjZve7+zKjt/uTubypiHcNiMSOrHBARGaFoLQJ33+7uj4f3u4A1wJxivd9ExHWFMhGRMSZljMDMmoEzgEcLrD7HzFaZ2V1mdtI4z7/SzJab2fKWlpaDrkNdQyIiYxU9CMysGvgZ8DF37xy1+nFgvrufBnwL+GWh13D3G919qbsvbWpqOuhaYppiQkRkjKIGgZklCULgh+7+89Hr3b3T3bvD+3cCSTObXqx64roegYjIGMU8asiAm4A17n71ONscFW6HmZ0Z1tNarJp0ZrGIyFjFPGroFcC7gKfMbGW47DPA0QDufgNwMXCVmWWAPuAS9+J9U+vi9SIiYxUtCNz9IcD2s821wLXFqmE0tQhERMaK3JnFukKZiMhIkQoCXaFMRGSsiAWBuoZEREaLVBBosFhEZKxIBYFaBCIiY0UrCExTTIiIjBapIIjpzGIRkTEiFQRxzTUkIjJGpIIgFtN5BCIio0UqCOIxdKlKEZFRohUEGiwWERkjUkGgwWIRkbEiFQRxMzIKAhGREaIVBDqhTERkjMgFgbqGRERGilwQqEUgIjJSpIIgZoY7FPEiaCIiU06kgiAeCy6YpkNIRUT2imYQqEUgIjIsUkEQsyAIcppmQkRkWKSCIB7urVoEIiJ7RSoIhloEGiMQEdkrUkEwNEagcwlERPYqWhCY2Twzu9/MnjGz1Wb20QLbmJldY2brzOxJM1tSrHpAg8UiIoUkivjaGeDj7v64mdUAK8zsXnd/Jm+bNwKLw9tZwPXhf4ti72CxgkBEZEjRWgTuvt3dHw/vdwFrgDmjNrsIuMUDjwD1ZjarWDWpRSAiMtakjBGYWTNwBvDoqFVzgM15j7cwNiwwsyvNbLmZLW9paTnoOuIaLBYRGaPoQWBm1cDPgI+5e+fBvIa73+juS919aVNT00HXEovpPAIRkdGKGgRmliQIgR+6+88LbLIVmJf3eG64rCh0HoGIyFjFPGrIgJuANe5+9Tib3QG8Ozx66Gygw923F6smnUcgIjJWMY8aegXwLuApM1sZLvsMcDSAu98A3AlcCKwDeoHLi1jP3vMI1CIQERlWtCBw94cA2882DnywWDWMpsFiEZGxInVmcTIcJBjMarRYRGRIpIIgVRYHoH9QQSAiMiRSQVCRDHa3bzBb4kpERA4fEQuCoEXQN6AgEBEZEqkgSIVBkM4oCEREhkw4CMxsvpm9LryfCieSm1LUIhARGWtCQWBm7wN+Cnw7XDQX+GWRaiqaoRaBxghERPaaaIvggwQniHUCuPvzwIxiFVUsOmpIRGSsiQZB2t0Hhh6YWQKYcmdllSd01JCIyGgTDYIHzewzQMrMzgd+Avy6eGUVh5lRkYzRryAQERk20SD4FNACPAW8n2COoH8tVlHFlErGFQQiInkmNNeQu+eA74S3Ka0iGddRQyIieSYUBGa2GPgycCJQMbTc3RcWqa6iSSXjGiMQEckz0a6h/yG4sHwGeA1wC/CDYhVVTBXJuI4aEhHJM9EgSLn7fYC5+wvu/nngb4pXVvFosFhEZKSJXo8gbWYx4Hkz+xDB5SSri1dW8aTKNFgsIpJvoi2CjwKVwEeAlwH/ALy7WEUVk8YIRERGmmiLwIFbgflAMlz2HeDUYhRVTOUKAhGRESYaBD8E/pngPIIpPdKaSsZJa7BYRGTYRIOgxd3vKGolk0RdQyIiI000CD5nZt8F7gPSQwvd/edFqaqIKpIxnVAmIpJnokFwOXA8wfjAUL+KA1MuCFLJOP2ZLO6OmZW6HBGRkptoEPyVux93IC9sZjcDbwJ2ufvJBdafC/wK2Bgu+rm7f/FA3uNgVJTFcYd0Jjd8oRoRkSib6OGjD5vZiQf42t8DLtjPNn9y99PDW9FDAKAiMXRNAnUPiYjAxFsEZwMrzWwjwRiBAe7u4x4+6u5/NLPml17ioaWL04iIjDTRINjfL/uDdY6ZrQK2AZ9w99VFep9hulyliMhIE52G+oUivPfjwHx37zazCwmugby40IZmdiVwJcDRRx/9kt60IhlepUxHDomIABMfIzjk3L3T3bvD+3cCSTObPs62N7r7Undf2tTU9JLet7o8ODG6O515Sa8jInKkKFkQmNlRFh6/aWZnhrW0Fvt96yuDIGjrHdjPliIi0TDRMYIDZmY/As4FppvZFuBzhPMUufsNwMXAVWaWAfqAS9zdi1XPkIaqMgDaFQQiIkARg8DdL93P+muBa4v1/uNpCFsEe3oGJ/utRUQOSyXrGiqVVDJOWSKmFoGISChyQWBmTKss0xiBiEgockEAwYBxW6+6hkREIKJB0FBZpq4hEZFQNIOgKsmeHgWBiAhENAjqK8toV9eQiAgQ0SCYVllGe98gk3DagojIYS+SQVBfmSSbczr7Nc2EiEgkg6ChUmcXi4gMiWQQTAunmWjVgLGISDSDYGZtBQA7OvpLXImISOlFMgjm1KcA2NbeV+JKRERKL5JBUJtKUFUWZ1u7WgQiIpEMAjNjdn1KLQIRESIaBEAQBB0KAhGRaAeBWgQiItENgjn1FezuHqB/UBexF5Foi2wQzA6PHFqzvbPElYiIlFZkg+DYmTWYwcU3/EVhICKRFtkgOHlOHbe972yyOWftjq5SlyMiUjKRDQKAU+bWAbBVg8YiEmGRDoLKsgT1lUm26zBSEYmwSAcBwOy6lM4wFpFIK1oQmNnNZrbLzJ4eZ72Z2TVmts7MnjSzJcWqZV9m11fofAIRibRitgi+B1ywj/VvBBaHtyuB64tYy7h0YpmIRF3RgsDd/wjs2ccmFwG3eOARoN7MZhWrnvHMqkvR2Z+hO62rlYlINJVyjGAOsDnv8ZZw2RhmdqWZLTez5S0tLYe0iNn1wbUJtqtVICIRNSUGi939Rndf6u5Lm5qaDulrz22oBGBTa+8hfV0RkamilEGwFZiX93huuGxSnTS7lrJ4jMc27asXS0TkyFXKILgDeHd49NDZQIe7b5/sIiqScU6bV8ejG1on+61FRA4LiWK9sJn9CDgXmG5mW4DPAUkAd78BuBO4EFgH9AKXF6uW/TlrQSPXP7ie7nSG6vKi/UlERA5LRfvWc/dL97PegQ8W6/0PxFkLp3Ht/etYvmkP5x43o9TliIhMqikxWFxsL5vfQCJmPLpx/+MEyzftoaNvcBKqEhGZHAoCgjmHTplbx7L9BMGa7Z1cfMNf+M+7n52kykREik9BEDprQSNPbmmnb2D8K5bd9NBGANKDuckqS0Sk6BQEobMWTmMw6zzxYlvB9W09A/xqZXB0azxmk1maiEhRKQhCp82tB2D1tsJXK/vd6h0MZh2Azn6NEYjIkUPHSoamVZUxs7acNTsKB8Fvn9zOgulV1Fcm6ezTvEQicuRQiyDPcUfV8uz2sZet7ElneHj9bi44+SjqUkm1CETkiKIgyHPCUTWs29XNYHbkYPDu7jQ5h4XTq6itSNKpw0dF5AiiIMhz/KwaBrI5Nu7uGbG8rTf44m+oLKM2laCzX11DInLkUBDkOW5mLQBrd4zsHmrrHQCgoSo53CIITowWEZn6FAR5FjZVAbChZWSLoD0MgvrKMmpTSTI5p29w/PMNRESmEgVBnopknDn1KTbu7h6xvK0nr2uoIgmgI4dE5IihIBhlYVMVG3aPbRGYQV0qSW0qOOJWRw6JyJFCQTDKgulVbGzpGTEG0NY7SG1FknjM8loECgIROTIoCEZZML2KrnSG3d0Dw8vaegeYVlUGQG0qDAK1CETkCKEgGGVhUzUAG1r2jhO09w5SXxkEQG1F2DWkMQIROUIoCEY5bmYNAKu2tA8va+sdoKFSLQIROTIpCEY5qq6C42bW8MDaluFlI1sESczgxdbeUpUoInJIKQgKOPf4JpZt3ENX+Ks/v0VQlohxwUlH8aNlL7K7O13KMkVEDgkFQQHnHTeDTM7587rdpDNZegeyNIQtAoBPvOE4+gaz3PKXF0pYpYjIoaEgKGDJ/AZSyTh/Wd/K5j19AMyorRhev6ipmiVHN/Dg2l2lKlFE5JBREBSQjMdY2tzAIxv2sHxTcB3jl81vGLHNqxY38eTWDvb0DBR6CRGRKUNBMI6zFzaydmcXd6/eQWNVGQunV41Y/9fHTscdHlq3u+Dz7169Y9zLXoqIHE6KGgRmdoGZrTWzdWb2qQLr32NmLWa2Mry9t5j1HIizFzYCcP/aFpbMb8Bs5HWKT51bT015gkc3tI55bi7nfOInq/jyXc9OSq0iIi9F0S5VaWZx4DrgfGAL8JiZ3eHuz4za9HZ3/1Cx6jhYZ8yr58JTjuLOp3bw8kWNY9bHY8Ypc+t4amvHiOVf/d2zxGNGV3+GlS+20z+YpSIZn6yyRUQOWDGvWXwmsM7dNwCY2W3ARcDoIDgsxWLGde9YwhOb2zllTl3BbU6ZW8fND20knclSnoiTzTn//cD64fUD2RyPv9jGyxdNn6yyRUQOWDG7huYAm/MebwmXjfZ3Zvakmf3UzOYVeiEzu9LMlpvZ8paWlkKbFIWZseToBpLxwn+mU+fUM5j14QvZbG3rG15XW5EgZvDIhj2TUquIyMEq9WDxr4Fmdz8VuBf4fqGN3P1Gd1/q7kubmpomtcB9OXVu0FK486kdDGZzrM+7jsE5ixo5ZU4dDz0/ecElInIwitk1tBXI/4U/N1w2zN3zR1q/C3y1iPUccnMbUpw6t44bHlxPOpNlTn0KgB9ccRYLm6q47bHNXPuH52nrGaAhnL30YLR0pVm7o4tXLlYXk4gcesVsETwGLDazBWZWBlwC3JG/gZnNynv4FmBNEes55MyMn1/1ct506ix+snwLq7d1Ul+Z5JWLpzO7PsVrjmsi5/DHl9gq+Nrda/mHmx7V/EYiUhRFCwJ3zwAfAu4m+IL/sbuvNrMvmtlbws0+YmarzWwV8BHgPcWqp1gS8RiXvbyZ7nSGXzyxlebGvecbnDq3nmlVZSMmsDtQg9kcdz+zA4Dbl7/4kusVERmtqGME7n6nux/r7ovc/Uvhss+6+x3h/U+7+0nufpq7v8bdp+SB90vnN/DqY4Oxi5Nm1w4vj8eMcxY18siG1hFXPDsQD69vpb13kOnVZdz+2BY6dGU0ETnESj1YfEQwM753+V9x50dexf95w/Ej1p21YBrbO/rZkndE0YG4b81OKsviXPeOJbT3DvDxH6886FARESlEQXCImBknzq6lLm+WUoAzF0wDYNnGgzuM9LFNbSw5uoGzFjby6QtP4PdrdnHHqm37fI67j7jCmojIvigIiuzYGTXUpZL8KW/A+Nkdndy9egf9g9l9Prezf5Bnd3SytDmY8O49L2/m1Ll1fOm3a+gbGP+5963ZxXlff5BVm9sPyT6IyJFNQVBksZjx1tNn86tV2/jZii10pzN84AeP8/5bV/Carz3Aw+NMWgfw+AttuMNfNQetinjM+JcLT2BXV5qfrtg87vN+v2YnAH9eP/5ri4gMURBMgk++8XgWTK/i4z9Zxbn/+QAbdvdw+SuaSSXjvOO7j/Llu9aQzY3t93/wuRbiMeP0efXDy85cMI3T59Vzw4MbePC5ljGHlLo7Dz4XtD4OtjtKRKJFQTAJKssS/PpDr+Tq/3Uau7vTVJbF+cTrj+O3H3kV7zjraL794AY+8MMVI7p7trX38cNHX+Si02ZTVb73vD8z45MXHE9n/yCX3byMV3/tfq67fx2f+cVT9A1keW5nN9s7+qlLJVmxqY1sLpgCQwPMIjKeYp5ZLHmqyhO8bclc0pkcMWP4y/3//u0pHNNUzb/99hneddOjfOsdZ7Cjo5+v3bMWHP7p9ceOea1zFjXyl0+/lidebOPffvMM/3n3WiCYMXV7Rz8AV527iP+461muf2AdX7vnOb78tlO49MyjJ2+HRWTKsKn2S3Hp0qW+fPnyUpdxyP161TY+ctsTAAx9JF97+2lc/LK5+3zext093LbsRe5evYOmmnI6+gapSyX5zruXcuaX7iPrTjbnTKsq4/6PnzvmqCYRiQYzW+HuSwutU4vgMPHm02bTnc7w2MY9nHfCDGbUVAwferovC6ZX8ekLT2BaVdnwhXC+8JaTqK8s4/UnzeQ3T27nzAXTWL5pD1ffu5YvXHQyAOlMlm8/uIGLXzaX2eEcSSISTQqCw8ilZx590N0373lFM139GZZt2sObT5sNwDvPms+dT23nkxccxy+f2Matj7zA8bNq+ZtTZ3Hbshe5+t7nWLZxDyfNqeUNJx3FkqMb9vMuInIkUtfQEW6oq6ijd5D33vIYj21qwwziZlSVJ4anrIjHjI++djFXnbsIA/68vpVT5tQxbR+zpj67o5OGyjJm1lYML7v6nrX0DWb5zIUnjLm8p4iUjrqGIqwuFYwJ1FUm+fH7z+Ev61tZ8UIbz+/q5sPnHcOvVm7jnEWN3P7YZq6+9zm+//AmAFp7Bjh6WiXX/8MSTppdRyabIx4zHl7fytX3PkfcjBUvtjF/WiV3fvRVVCTjtPUMcP2D6xnMOqlknNccP4MTZ9dSnth7qc67ntrOH5/fzYfOO4a7ntrOPc/s5NpLz2BGXpiIyORSi0CA4PyDB9a28KuVW0nEY5w6t47/+v3ztPYM0NxYybaOfmrKE7T2DDCnPkU258yqr+CJF9s5bV49bztjDj0DGb76u7WcPKeWp7d2ArB4RjXXXHoG/YNZ5jdWcf7VD9LaM0BFMkY6k8MdaioSLJ3fwAdfcwyN1eU0N1aOaU1kc053OjMcbCJyYPbVIlAQyLg6egf5waMvsHpbBzNqKtjTM8AJs2q5/BXNlCeCU1BuemgjP12xhWfDy3WePKeWOz74Sja39bJs4x4+84unGMwG/8ZiBjmHay49g7tX72BXZz8ff/1x/OLxrfxu9Y7hbqo59SnecvpsFkyvYk/PAM/t7OL+Z3fR3jfIe1+5gPrKMt62ZA4zairY0NKNGTQ3VuEw7mVFD1Z77wDliTipsvj+NxY5jCkIpKjcnUc27GFLWy9nL2xk3rTK4XX3rdnJszu6WNRUzYPPtVBbkeDTF54w5jVautI8urGVzr4Mdz29nT+v283QydYNlUlefWwTvQNZ7nkmmD4jHjMqk3G60hkAknFjMOssaqpienU561t66B3I0NxYRd9glpauNAumVxGPGZ39gyxorKIsEaOtd4Cmmgqe3trB/MZKzlnYSHkixszaCtZs7+S7D22ktiLJP56/mAXTqzGDpupyutMZ1mzvZFdXmpm1FRw7s5regSzV5Qkaq8uoSyXpH8xRkYxRkYiTcyeTcyqSIwOlfzDL5j29LGqqJudO4hAH2Xie29lFbUWSo+rUJRcVCgKZcvoGsuzq6qexupzq8OQ7d2dbRz+ZbI6frdhCS/cAL5vfgBF8sZUlYqzZ3sWenjQLm6qpLk+wcXcP1RUJplWWsb6lm3jMqC5PsHZnFzEz6lNJtrT10Ty9kud3dtPaMzCijtedMIOt7f2s2d75kvdpKERqU0kaKpM8t7N7uMurLB4j586C6VVsa+8jVZYgPZjl7EWNGNCdzlBdnqCyLM7W9mBK8/JEnPJEjPJkjLJ4LHicjBEz44XWHlp7BphZW8EJR9VQm0rS3hu0uAazOW56aCMxM95y+mxeNr+BulSSmooELV1pntragWEsmlFFQ2UZ9ZVJ6lNlrNzczo7Ofo6dWU1r9wA9AxkWz6ihvjLJlrZe+gZyHD2tklgMuvszbGrtIZNzjp1Rw8zaCl7c08tgNsfchhTbO/qZXV+BmWFAfWUZOzv7Gcjkgr9TTTmDGadnINjvhqoy3J0XWnuZWVtBeSLGC629JONGY3U5nf2DxMyoqUhQXZ5gS1svdakkTdUVDGRz5MLvuaGvO2fv914qGadvMEvfQJa6VJKcQ1kiRnkiRt9AlmQiRioZx4CW7jQDmRyZXHB+TjJuJOMxEnGjqiz4fFp7BmjtHmBRU9Vw+Ls7u7sHiMds+ACMXM7Z2dVPejBHVXmCulSSsrClvauzn6ryBOlMLjywI07Og7rzx9wO7N+fgkBkv3I5p28wSzqTY/OeXmbUljOrLoW78/TWTtp6g5DY1ZUmlYxzwqwaZteneHZHFzs6+qmpSNCdzrCrK013f2Z4HKR/MDv8BbS9o4/W7gF2daU5eU4dZsHFjDa09GDA87u6aW6sJJ3Jkc05yzbtIZWMU1WeoLs/Q3c6w5yGFImYkc7kSGeypAdzDGRzpAeDx4NZZ25Dihm1FWxt62Xj7h5yHrSi3J2cw2uOa2JWfYo7Vm6jO2xVDUkl4zhO/2BuzN/IbO+XaZSUJWJUJGJ09mf2uV1lWZzecKqYoa7QulSSdCY7/PecVlVG70CGwayPmWOspjxBVXmCHZ39BV//A+cu4v9ccHzBdfujo4ZEJiAWCw6prSpnxGGzZsYpc+vGfd7p8+ph3iQUeJDSmSz9AzmqKxLEDNKZ3HAX1efefCKt3QN09g/S0TtIQ1XZcDdVa/cAbb0DtPcO0t47wIzachbPrGFbex8NlWVUlsV5flc3HX2DzGuopDwRY1t7HzmHimSMRTOqh1trOzvTNDcGXXOb9/Qyq76CXV1pDIbf66i6ClLJ4Jfvrq5+yhNxKsvidPVn6OgbwB3mNlSys7OfbC4Iu2z43JqKBGbQ1Z+hsz/DzJpy2nsH6UpnKIsb8djeLreh4xCGDkfoTmcoS8SoKkvQ1T9IPGb0h+GaSsbJ5HK0dKXpTmc5cVYNFck4yXgMs+AghsFsjsGs09k/yK7ONPMbK6lLJVm3q5uKZJzd3WnK4jHmNqTI5Jznd3ZTU5GgPBljVl2KVDJOz0CGjt5BWnsGaO8NxuIyOQ9DOWhhxQyWNu//JNODoRaBiEgE7KtFoNlHRUQiTkEgIhJxCgIRkYhTEIiIRFxRg8DMLjCztWa2zsw+VWB9uZndHq5/1Myai1mPiIiMVbQgMLM4cB3wRuBE4FIzO3HUZlcAbe5+DPAN4CvFqkdERAorZovgTGCdu29w9wHgNuCiUdtcBHw/vP9T4LWmuYtFRCZVMYNgDrA57/GWcFnBbdw9A3QAjaNfyMyuNLPlZra8paWlSOWKiETTlDiz2N1vBG4EMLMWM3vhIF9qOrD7kBVWWtqXw5P25fCkfYH5460oZhBsZeSJ93PDZYW22WJmCaAOaN3Xi7p708EWZGbLxzuzbqrRvhyetC+HJ+3LvhWza+gxYLGZLTCzMuAS4I5R29wBXBbevxj4g0+1OS9ERKa4orUI3D1jZh8C7gbiwM3uvtrMvggsd/c7gJuAW81sHbCHICxERGQSFXWMwN3vBO4cteyzeff7gbcXs4ZRbpzE9yo27cvhSftyeNK+7MOUm31UREQOLU0xISIScQoCEZGIi0wQ7G/eo8OdmW0ys6fMbKWZLQ+XTTOze83s+fC/DaWusxAzu9nMdpnZ03nLCtZugWvCz+lJM1tSusrHGmdfPm9mW8PPZqWZXZi37tPhvqw1szeUpuqxzGyemd1vZs+Y2Woz+2i4fMp9LvvYl6n4uVSY2TIzWxXuyxfC5QvC+djWhfOzlYXLD818be5+xN8IjlpaDywEyoBVwImlrusA92ETMH3Usq8Cnwrvfwr4SqnrHKf2vwaWAE/vr3bgQuAugisJng08Wur6J7Avnwc+UWDbE8N/a+XAgvDfYLzU+xDWNgtYEt6vAZ4L651yn8s+9mUqfi4GVIf3k8Cj4d/7x8Al4fIbgKvC+x8AbgjvXwLcfjDvG5UWwUTmPZqK8udq+j7w1tKVMj53/yPB4cH5xqv9IuAWDzwC1JvZrEkpdALG2ZfxXATc5u5pd98IrCP4t1hy7r7d3R8P73cBawimfJlyn8s+9mU8h/Pn4u7eHT5MhjcHziOYjw3Gfi4veb62qATBROY9Otw5cI+ZrTCzK8NlM919e3h/BzCzNKUdlPFqn6qf1YfCLpOb87ropsS+hN0JZxD8+pzSn8uofYEp+LmYWdzMVgK7gHsJWiztHszHBiPrndB8bfsTlSA4ErzS3ZcQTOv9QTP76/yVHrQNp+SxwFO59tD1wCLgdGA78PWSVnMAzKwa+BnwMXfvzF831T6XAvsyJT8Xd8+6++kE0/KcCRxf7PeMShBMZN6jw5q7bw3/uwv4BcE/kJ1DzfPwv7tKV+EBG6/2KfdZufvO8H/eHPAd9nYzHNb7YmZJgi/OH7r7z8PFU/JzKbQvU/VzGeLu7cD9wDkEXXFDJwDn1zu8LzbB+doKiUoQTGTeo8OWmVWZWc3QfeD1wNOMnKvpMuBXpanwoIxX+x3Au8OjVM4GOvK6Kg5Lo/rK/5bgs4FgXy4Jj+xYACwGlk12fYWE/cg3AWvc/eq8VVPucxlvX6bo59JkZvXh/RRwPsGYx/0E87HB2M/lpc/XVupR8sm6ERz18BxBf9u/lLqeA6x9IcFRDquA1UP1E/QF3gc8D/wemFbqWsep/0cETfNBgv7NK8arneCoievCz+kpYGmp65/Avtwa1vpk+D/mrLzt/yXcl7XAG0tdf15dryTo9nkSWBneLpyKn8s+9mUqfi6nAk+ENT8NfDZcvpAgrNYBPwHKw+UV4eN14fqFB/O+mmJCRCTiotI1JCIi41AQiIhEnIJARCTiFAQiIhGnIBARiTgFgcgoZpbNm7FypR3C2WrNrDl/5lKRw0FRL1UpMkX1eXCKv0gkqEUgMkEWXBPiqxZcF2KZmR0TLm82sz+Ek5vdZ2ZHh8tnmtkvwrnlV5nZy8OXipvZd8L55u8JzyAVKRkFgchYqVFdQ3+ft67D3U8BrgW+GS77FvB9dz8V+CFwTbj8GuBBdz+N4BoGq8Pli4Hr3P0koB34u6Lujch+6MxikVHMrNvdqwss3wSc5+4bwknOdrh7o5ntJpi+YDBcvt3dp5tZCzDX3dN5r9EM3Ovui8PHnwSS7v7vk7BrIgWpRSByYHyc+wcinXc/i8bqpMQUBCIH5u/z/vuX8P7DBDPaArwT+FN4/z7gKhi+2EjdZBUpciD0S0RkrFR4haghv3P3oUNIG8zsSYJf9ZeGyz4M/I+Z/TPQAlweLv8ocKOZXUHwy/8qgplLRQ4rGiMQmaBwjGCpu+8udS0ih5K6hkREIk4tAhGRiFOLQEQk4hQEIiIRpyAQEYk4BYGISMQpCEREIu7/A1+mhI9OSHrxAAAAAElFTkSuQmCC\n"},"metadata":{"needs_background":"light"}}]}]}